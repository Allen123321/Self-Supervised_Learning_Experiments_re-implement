{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BYOL-TensorFlow 2.ipynb",
      "provenance": [],
      "mount_file_id": "1C8k028FyTEl019u9LEi1SR_MGcJwxAzX",
      "authorship_tag": "ABX9TyM1QYvgqck+5uMqrbO/FX1F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Allen123321/Self-Supervised_Learning_Experiments_re-implement/blob/main/BYOL_TensorFlow_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGCCOQ50_k9q"
      },
      "source": [
        "BYOL for self-supervised representation learning on the CIFAR-10 dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jP1oojoOCD9j"
      },
      "source": [
        "## data-argumentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hm6e1WyVBmw1"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "\r\n",
        "def random_crop_flip_resize(image):\r\n",
        "    # Random cropping\r\n",
        "    h_crop = tf.cast(tf.random.uniform(shape=[], minval=13, maxval=33, dtype=tf.int32), tf.float32)\r\n",
        "    w_crop = h_crop * tf.random.uniform(shape=[], minval=0.67, maxval=1.0)\r\n",
        "    h_crop, w_crop = tf.cast(h_crop, tf.int32), tf.cast(w_crop, tf.int32)\r\n",
        "    opposite_aspectratio = tf.random.uniform(shape=[])\r\n",
        "    if opposite_aspectratio < 0.5:\r\n",
        "        h_crop, w_crop = w_crop, h_crop\r\n",
        "    image = tf.image.random_crop(image, size=[h_crop, w_crop, 3])\r\n",
        "\r\n",
        "    # Horizontal flipping\r\n",
        "    horizontal_flip = tf.random.uniform(shape=[])\r\n",
        "    if horizontal_flip < 0.5:\r\n",
        "        image = tf.image.random_flip_left_right(image)\r\n",
        "\r\n",
        "    # Resizing to original size\r\n",
        "    image = tf.image.resize(image, size=[32, 32])\r\n",
        "    return image\r\n",
        "\r\n",
        "\r\n",
        "def random_color_distortion(image):\r\n",
        "    # Random color jittering (strength 0.5)\r\n",
        "    color_jitter = tf.random.uniform(shape=[])\r\n",
        "    if color_jitter < 0.8:\r\n",
        "        image = tf.image.random_brightness(image, max_delta=0.4)\r\n",
        "        image = tf.image.random_contrast(image, lower=0.6, upper=1.4)\r\n",
        "        image = tf.image.random_saturation(image, lower=0.6, upper=1.4)\r\n",
        "        image = tf.image.random_hue(image, max_delta=0.1)\r\n",
        "        image = tf.clip_by_value(image, 0, 1)\r\n",
        "\r\n",
        "    # Color dropping\r\n",
        "    color_drop = tf.random.uniform(shape=[])\r\n",
        "    if color_drop < 0.2:\r\n",
        "        image = tf.image.rgb_to_grayscale(image)\r\n",
        "        image = tf.tile(image, [1, 1, 3])\r\n",
        "\r\n",
        "    return image\r\n",
        "\r\n",
        "\r\n",
        "@tf.function\r\n",
        "def augment_image_pretraining(image):\r\n",
        "    image = random_crop_flip_resize(image)\r\n",
        "    image = random_color_distortion(image)\r\n",
        "    return image\r\n",
        "\r\n",
        "\r\n",
        "@tf.function\r\n",
        "def augment_image_finetuning(image):\r\n",
        "    image = random_crop_flip_resize(image)\r\n",
        "    return image"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kciiw4r5CXh7"
      },
      "source": [
        "## datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_zLSlcQCTs9"
      },
      "source": [
        "class CIFAR10:\r\n",
        "\r\n",
        "    def __init__(self):\r\n",
        "        (self.x_train, self.y_train), (self.x_test, self.y_test) = tf.keras.datasets.cifar10.load_data()\r\n",
        "        self.num_train_images, self.num_test_images = self.y_train.shape[0], self.y_test.shape[0]\r\n",
        "        self.class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'sheep', 'truck']\r\n",
        "\r\n",
        "        # Normalize training and testing images\r\n",
        "        self.x_train = tf.cast(self.x_train / 255., tf.float32)\r\n",
        "        self.x_test = tf.cast(self.x_test / 255., tf.float32)\r\n",
        "\r\n",
        "        self.y_train = tf.cast(tf.squeeze(self.y_train), tf.int32)\r\n",
        "        self.y_test = tf.cast(tf.squeeze(self.y_test), tf.int32)\r\n",
        "\r\n",
        "\r\n",
        "    def get_batch_pretraining(self, batch_id, batch_size):\r\n",
        "        augmented_images_1, augmented_images_2 = [], []\r\n",
        "        for image_id in range(batch_id*batch_size, (batch_id+1)*batch_size):\r\n",
        "            image = self.x_train[image_id]\r\n",
        "            augmented_images_1.append(augment_image_pretraining(image))\r\n",
        "            augmented_images_2.append(augment_image_pretraining(image))\r\n",
        "        x_batch_1 = tf.stack(augmented_images_1)\r\n",
        "        x_batch_2 = tf.stack(augmented_images_2)\r\n",
        "        return x_batch_1, x_batch_2  # (bs, 32, 32, 3), (bs, 32, 32, 3)\r\n",
        "\r\n",
        "\r\n",
        "    def get_batch_finetuning(self, batch_id, batch_size):\r\n",
        "        augmented_images = []\r\n",
        "        for image_id in range(batch_id*batch_size, (batch_id+1)*batch_size):\r\n",
        "            image = self.x_train[image_id]\r\n",
        "            augmented_images.append(augment_image_finetuning(image))\r\n",
        "        x_batch = tf.stack(augmented_images)\r\n",
        "        y_batch = tf.slice(self.y_train, [batch_id*batch_size], [batch_size])\r\n",
        "        return x_batch, y_batch  # (bs, 32, 32, 3), (bs)\r\n",
        "\r\n",
        "\r\n",
        "    def get_batch_testing(self, batch_id, batch_size):\r\n",
        "        x_batch = tf.slice(self.x_test, [batch_id*batch_size, 0, 0, 0], [batch_size, -1, -1, -1])\r\n",
        "        y_batch = tf.slice(self.y_test, [batch_id*batch_size], [batch_size])\r\n",
        "        return x_batch, y_batch  # (bs, 32, 32, 3), (bs)\r\n",
        "\r\n",
        "\r\n",
        "    def shuffle_training_data(self):\r\n",
        "        random_ids = tf.random.shuffle(tf.range(self.num_train_images))\r\n",
        "        self.x_train = tf.gather(self.x_train, random_ids)\r\n",
        "        self.y_train = tf.gather(self.y_train, random_ids)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsS9x7xjC3ig"
      },
      "source": [
        "## loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AX3o7DRRCu9g"
      },
      "source": [
        "\r\n",
        "def byol_loss(p, z):\r\n",
        "    p = tf.math.l2_normalize(p, axis=1)  # (2*bs, 128)\r\n",
        "    z = tf.math.l2_normalize(z, axis=1)  # (2*bs, 128)\r\n",
        "\r\n",
        "    similarities = tf.reduce_sum(tf.multiply(p, z), axis=1)\r\n",
        "    return 2 - 2 * tf.reduce_mean(similarities)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mznOD2E6C-Hy"
      },
      "source": [
        "## model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiiV8yQyDApv"
      },
      "source": [
        "class BasicBlock(tf.keras.layers.Layer):\r\n",
        "\r\n",
        "    def __init__(self, filters, strides):\r\n",
        "        super(BasicBlock, self).__init__()\r\n",
        "        self.conv1 = tf.keras.layers.Conv2D(filters=filters, kernel_size=(3, 3), strides=strides, padding='same')\r\n",
        "        self.bn1 = tf.keras.layers.BatchNormalization()\r\n",
        "        self.conv2 = tf.keras.layers.Conv2D(filters=filters, kernel_size=(3, 3), strides=1, padding='same')\r\n",
        "        self.bn2 = tf.keras.layers.BatchNormalization()\r\n",
        "\r\n",
        "        if strides != 1:\r\n",
        "            self.convdown = tf.keras.layers.Conv2D(filters=filters, kernel_size=(1, 1), strides=strides)\r\n",
        "            self.bndown = tf.keras.layers.BatchNormalization()\r\n",
        "        self.strides = strides\r\n",
        "\r\n",
        "    def call(self, inp, training=False):\r\n",
        "        x1 = self.conv1(inp)\r\n",
        "        x1 = self.bn1(x1, training=training)\r\n",
        "        x1 = tf.nn.relu(x1)\r\n",
        "        x1 = self.conv2(x1)\r\n",
        "        x1 = self.bn2(x1, training=training)\r\n",
        "\r\n",
        "        if self.strides != 1:\r\n",
        "            x2 = self.convdown(inp)\r\n",
        "            x2 = self.bndown(x2, training=training)\r\n",
        "        else:\r\n",
        "            x2 = inp\r\n",
        "\r\n",
        "        x = tf.keras.layers.add([x1, x2])\r\n",
        "        x = tf.nn.relu(x)\r\n",
        "        return x\r\n",
        "\r\n",
        "\r\n",
        "# ResNet with BasicBlock (adapted to CIFAR-10)\r\n",
        "class BasicResNet(tf.keras.Model):\r\n",
        "\r\n",
        "    def __init__(self, layer_blocks):\r\n",
        "        super(BasicResNet, self).__init__()\r\n",
        "\r\n",
        "        self.conv1 = tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), strides=1, padding='same')\r\n",
        "        self.bn1 = tf.keras.layers.BatchNormalization()\r\n",
        "\r\n",
        "        self.blocks = []\r\n",
        "        self.blocks.append(BasicBlock(filters=64, strides=1))\r\n",
        "        for _ in range(layer_blocks[0] - 1):\r\n",
        "            self.blocks.append(BasicBlock(filters=64, strides=1))\r\n",
        "        \r\n",
        "        self.blocks.append(BasicBlock(filters=128, strides=2))\r\n",
        "        for _ in range(layer_blocks[1] - 1):\r\n",
        "            self.blocks.append(BasicBlock(filters=128, strides=1))\r\n",
        "        \r\n",
        "        self.blocks.append(BasicBlock(filters=256, strides=2))\r\n",
        "        for _ in range(layer_blocks[2] - 1):\r\n",
        "            self.blocks.append(BasicBlock(filters=256, strides=1))\r\n",
        "        \r\n",
        "        self.blocks.append(BasicBlock(filters=512, strides=2))\r\n",
        "        for _ in range(layer_blocks[3] - 1):\r\n",
        "            self.blocks.append(BasicBlock(filters=512, strides=1))\r\n",
        "\r\n",
        "        self.avgpool = tf.keras.layers.GlobalAveragePooling2D()\r\n",
        "\r\n",
        "    def call(self, inp, training=False):\r\n",
        "        x = self.conv1(inp)\r\n",
        "        x = self.bn1(x, training=training)\r\n",
        "        x = tf.nn.relu(x)\r\n",
        "        for block in self.blocks:\r\n",
        "            x = block(x, training=training)\r\n",
        "        x = self.avgpool(x)\r\n",
        "        return x\r\n",
        "\r\n",
        "\r\n",
        "def ResNet18():\r\n",
        "    return BasicResNet(layer_blocks=[2, 2, 2, 2])\r\n",
        "\r\n",
        "\r\n",
        "def ResNet34():\r\n",
        "    return BasicResNet(layer_blocks=[3, 4, 6, 3])\r\n",
        "\r\n",
        "\r\n",
        "# 512 (h) -> 256 -> 128 (z)\r\n",
        "class ProjectionHead(tf.keras.Model):\r\n",
        "\r\n",
        "    def __init__(self):\r\n",
        "        super(ProjectionHead, self).__init__()\r\n",
        "        self.fc1 = tf.keras.layers.Dense(units=256)\r\n",
        "        self.bn = tf.keras.layers.BatchNormalization()\r\n",
        "        self.fc2 = tf.keras.layers.Dense(units=128)\r\n",
        "\r\n",
        "    def call(self, inp, training=False):\r\n",
        "        x = self.fc1(inp)\r\n",
        "        x = self.bn(x, training=training)\r\n",
        "        x = tf.nn.relu(x)\r\n",
        "        x = self.fc2(x)\r\n",
        "        return x\r\n",
        "\r\n",
        "\r\n",
        "# 512 (h) -> 10 (s)\r\n",
        "class ClassificationHead(tf.keras.Model):\r\n",
        "\r\n",
        "    def __init__(self):\r\n",
        "        super(ClassificationHead, self).__init__()\r\n",
        "        self.fc = tf.keras.layers.Dense(units=10)\r\n",
        "\r\n",
        "    def call(self, inp):\r\n",
        "        x = self.fc(inp)\r\n",
        "        return x"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmtnKp3aGYCf"
      },
      "source": [
        "### pre-train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jebXnwm0DbRD"
      },
      "source": [
        "import os\r\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\n",
        "\r\n",
        "#import argparse\r\n",
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "#from datasets import CIFAR10\r\n",
        "#from models import ResNet18, ResNet34, ProjectionHead\r\n",
        "#from losses import byol_loss\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "encoders = {'resnet18': ResNet18, 'resnet34': ResNet34}\r\n",
        "\r\n",
        "\r\n",
        "def train(encoder,num_epochs,batch_size):\r\n",
        "\r\n",
        "    # Load CIFAR-10 dataset\r\n",
        "    data = CIFAR10()\r\n",
        "\r\n",
        "    # Instantiate networks\r\n",
        "    f_online = encoders[encoder]()\r\n",
        "    g_online = ProjectionHead()\r\n",
        "    q_online = ProjectionHead()\r\n",
        "\r\n",
        "    f_target = encoders[encoder]()\r\n",
        "    g_target = ProjectionHead()\r\n",
        "\r\n",
        "\r\n",
        "    # Initialize the weights of the networks\r\n",
        "    x = tf.random.normal((256, 32, 32, 3))\r\n",
        "    h = f_online(x, training=False)\r\n",
        "    print('Initializing online networks...')\r\n",
        "    print('Shape of h:', h.shape)\r\n",
        "    z = g_online(h, training=False)\r\n",
        "    print('Shape of z:', z.shape)\r\n",
        "    p = q_online(z, training=False)\r\n",
        "    print('Shape of p:', p.shape)\r\n",
        "\r\n",
        "    h = f_target(x, training=False)\r\n",
        "    print('Initializing target networks...')\r\n",
        "    print('Shape of h:', h.shape)\r\n",
        "    z = g_target(h, training=False)\r\n",
        "    print('Shape of z:', z.shape)\r\n",
        "    \r\n",
        "    num_params_f = tf.reduce_sum([tf.reduce_prod(var.shape) for var in f_online.trainable_variables])    \r\n",
        "    print('The encoders have {} trainable parameters each.'.format(num_params_f))\r\n",
        "\r\n",
        "\r\n",
        "    # Define optimizer\r\n",
        "    lr = 1e-3 * batch_size / 512\r\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=lr)\r\n",
        "    print('Using Adam optimizer with learning rate {}.'.format(lr))\r\n",
        "\r\n",
        "\r\n",
        "    @tf.function\r\n",
        "    def train_step_pretraining(x1, x2):  # (bs, 32, 32, 3), (bs, 32, 32, 3)\r\n",
        "\r\n",
        "        # Forward pass\r\n",
        "        h_target_1 = f_target(x1, training=True)\r\n",
        "        z_target_1 = g_target(h_target_1, training=True)\r\n",
        "\r\n",
        "        h_target_2 = f_target(x2, training=True)\r\n",
        "        z_target_2 = g_target(h_target_2, training=True)\r\n",
        "\r\n",
        "        with tf.GradientTape(persistent=True) as tape:\r\n",
        "            h_online_1 = f_online(x1, training=True)\r\n",
        "            z_online_1 = g_online(h_online_1, training=True)\r\n",
        "            p_online_1 = q_online(z_online_1, training=True)\r\n",
        "            \r\n",
        "            h_online_2 = f_online(x2, training=True)\r\n",
        "            z_online_2 = g_online(h_online_2, training=True)\r\n",
        "            p_online_2 = q_online(z_online_2, training=True)\r\n",
        "            \r\n",
        "            p_online = tf.concat([p_online_1, p_online_2], axis=0)\r\n",
        "            z_target = tf.concat([z_target_2, z_target_1], axis=0)\r\n",
        "            loss = byol_loss(p_online, z_target)\r\n",
        "\r\n",
        "        # Backward pass (update online networks)\r\n",
        "        grads = tape.gradient(loss, f_online.trainable_variables)\r\n",
        "        opt.apply_gradients(zip(grads, f_online.trainable_variables))\r\n",
        "        grads = tape.gradient(loss, g_online.trainable_variables)\r\n",
        "        opt.apply_gradients(zip(grads, g_online.trainable_variables))\r\n",
        "        grads = tape.gradient(loss, q_online.trainable_variables)\r\n",
        "        opt.apply_gradients(zip(grads, q_online.trainable_variables))\r\n",
        "        del tape\r\n",
        "\r\n",
        "        return loss\r\n",
        "\r\n",
        "\r\n",
        "    batches_per_epoch = data.num_train_images // batch_size\r\n",
        "    log_every = 10  # batches\r\n",
        "    save_every = 100  # epochs\r\n",
        "\r\n",
        "    losses = []\r\n",
        "    for epoch_id in range(num_epochs):\r\n",
        "        data.shuffle_training_data()\r\n",
        "        \r\n",
        "        for batch_id in range(batches_per_epoch):\r\n",
        "            x1, x2 = data.get_batch_pretraining(batch_id, batch_size)\r\n",
        "            loss = train_step_pretraining(x1, x2)\r\n",
        "            losses.append(float(loss))\r\n",
        "\r\n",
        "            # Update target networks (exponential moving average of online networks)\r\n",
        "            beta = 0.99\r\n",
        "\r\n",
        "            f_target_weights = f_target.get_weights()\r\n",
        "            f_online_weights = f_online.get_weights()\r\n",
        "            for i in range(len(f_online_weights)):\r\n",
        "                f_target_weights[i] = beta * f_target_weights[i] + (1 - beta) * f_online_weights[i]\r\n",
        "            f_target.set_weights(f_target_weights)\r\n",
        "            \r\n",
        "            g_target_weights = g_target.get_weights()\r\n",
        "            g_online_weights = g_online.get_weights()\r\n",
        "            for i in range(len(g_online_weights)):\r\n",
        "                g_target_weights[i] = beta * g_target_weights[i] + (1 - beta) * g_online_weights[i]\r\n",
        "            g_target.set_weights(g_target_weights)\r\n",
        "\r\n",
        "            if (batch_id + 1) % log_every == 0:\r\n",
        "                print('[Epoch {}/{} Batch {}/{}] Loss={:.5f}.'.format(epoch_id+1, num_epochs, batch_id+1, batches_per_epoch, loss))\r\n",
        "\r\n",
        "        if (epoch_id + 1) % save_every == 0:\r\n",
        "            f_online.save_weights('/content/drive/MyDrive/dataset/f_online_{}.h5'.format(epoch_id + 1))\r\n",
        "            print('Weights of f saved.')\r\n",
        "    \r\n",
        "    np.savetxt('losses.txt', tf.stack(losses).numpy())\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amsmegjDGjiE"
      },
      "source": [
        "预训练"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zux3TLXvEAXO",
        "outputId": "f18ea6f2-f4e6-465f-df8c-28100a37eb69"
      },
      "source": [
        "train(\"resnet18\",200,512)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 5s 0us/step\n",
            "Initializing online networks...\n",
            "Shape of h: (256, 512)\n",
            "Shape of z: (256, 128)\n",
            "Shape of p: (256, 128)\n",
            "Initializing target networks...\n",
            "Shape of h: (256, 512)\n",
            "Shape of z: (256, 128)\n",
            "The encoders have 11173632 trainable parameters each.\n",
            "Using Adam optimizer with learning rate 0.001.\n",
            "[Epoch 1/200 Batch 10/97] Loss=0.81563.\n",
            "[Epoch 1/200 Batch 20/97] Loss=0.67043.\n",
            "[Epoch 1/200 Batch 30/97] Loss=0.57101.\n",
            "[Epoch 1/200 Batch 40/97] Loss=0.54568.\n",
            "[Epoch 1/200 Batch 50/97] Loss=0.47895.\n",
            "[Epoch 1/200 Batch 60/97] Loss=0.40210.\n",
            "[Epoch 1/200 Batch 70/97] Loss=0.24860.\n",
            "[Epoch 1/200 Batch 80/97] Loss=0.24036.\n",
            "[Epoch 1/200 Batch 90/97] Loss=0.14926.\n",
            "[Epoch 2/200 Batch 10/97] Loss=0.13301.\n",
            "[Epoch 2/200 Batch 20/97] Loss=0.14967.\n",
            "[Epoch 2/200 Batch 30/97] Loss=0.14494.\n",
            "[Epoch 2/200 Batch 40/97] Loss=0.13782.\n",
            "[Epoch 2/200 Batch 50/97] Loss=0.13844.\n",
            "[Epoch 2/200 Batch 60/97] Loss=0.13403.\n",
            "[Epoch 2/200 Batch 70/97] Loss=0.14207.\n",
            "[Epoch 2/200 Batch 80/97] Loss=0.13806.\n",
            "[Epoch 2/200 Batch 90/97] Loss=0.13551.\n",
            "[Epoch 3/200 Batch 10/97] Loss=0.16977.\n",
            "[Epoch 3/200 Batch 20/97] Loss=0.13805.\n",
            "[Epoch 3/200 Batch 30/97] Loss=0.14471.\n",
            "[Epoch 3/200 Batch 40/97] Loss=0.16812.\n",
            "[Epoch 3/200 Batch 50/97] Loss=0.15471.\n",
            "[Epoch 3/200 Batch 60/97] Loss=0.19867.\n",
            "[Epoch 3/200 Batch 70/97] Loss=0.16559.\n",
            "[Epoch 3/200 Batch 80/97] Loss=0.18009.\n",
            "[Epoch 3/200 Batch 90/97] Loss=0.19953.\n",
            "[Epoch 4/200 Batch 10/97] Loss=0.19776.\n",
            "[Epoch 4/200 Batch 20/97] Loss=0.21348.\n",
            "[Epoch 4/200 Batch 30/97] Loss=0.22705.\n",
            "[Epoch 4/200 Batch 40/97] Loss=0.24544.\n",
            "[Epoch 4/200 Batch 50/97] Loss=0.27812.\n",
            "[Epoch 4/200 Batch 60/97] Loss=0.25676.\n",
            "[Epoch 4/200 Batch 70/97] Loss=0.25356.\n",
            "[Epoch 4/200 Batch 80/97] Loss=0.28205.\n",
            "[Epoch 4/200 Batch 90/97] Loss=0.27071.\n",
            "[Epoch 5/200 Batch 10/97] Loss=0.26084.\n",
            "[Epoch 5/200 Batch 20/97] Loss=0.27106.\n",
            "[Epoch 5/200 Batch 30/97] Loss=0.27298.\n",
            "[Epoch 5/200 Batch 40/97] Loss=0.28229.\n",
            "[Epoch 5/200 Batch 50/97] Loss=0.31826.\n",
            "[Epoch 5/200 Batch 60/97] Loss=0.31088.\n",
            "[Epoch 5/200 Batch 70/97] Loss=0.28515.\n",
            "[Epoch 5/200 Batch 80/97] Loss=0.33521.\n",
            "[Epoch 5/200 Batch 90/97] Loss=0.31092.\n",
            "[Epoch 6/200 Batch 10/97] Loss=0.31836.\n",
            "[Epoch 6/200 Batch 20/97] Loss=0.35373.\n",
            "[Epoch 6/200 Batch 30/97] Loss=0.31113.\n",
            "[Epoch 6/200 Batch 40/97] Loss=0.33218.\n",
            "[Epoch 6/200 Batch 50/97] Loss=0.35093.\n",
            "[Epoch 6/200 Batch 60/97] Loss=0.37724.\n",
            "[Epoch 6/200 Batch 70/97] Loss=0.35663.\n",
            "[Epoch 6/200 Batch 80/97] Loss=0.32617.\n",
            "[Epoch 6/200 Batch 90/97] Loss=0.34488.\n",
            "[Epoch 7/200 Batch 10/97] Loss=0.33131.\n",
            "[Epoch 7/200 Batch 20/97] Loss=0.36134.\n",
            "[Epoch 7/200 Batch 30/97] Loss=0.38494.\n",
            "[Epoch 7/200 Batch 40/97] Loss=0.38069.\n",
            "[Epoch 7/200 Batch 50/97] Loss=0.40366.\n",
            "[Epoch 7/200 Batch 60/97] Loss=0.36688.\n",
            "[Epoch 7/200 Batch 70/97] Loss=0.36738.\n",
            "[Epoch 7/200 Batch 80/97] Loss=0.38695.\n",
            "[Epoch 7/200 Batch 90/97] Loss=0.36779.\n",
            "[Epoch 8/200 Batch 10/97] Loss=0.39566.\n",
            "[Epoch 8/200 Batch 20/97] Loss=0.39599.\n",
            "[Epoch 8/200 Batch 30/97] Loss=0.35943.\n",
            "[Epoch 8/200 Batch 40/97] Loss=0.36777.\n",
            "[Epoch 8/200 Batch 50/97] Loss=0.37169.\n",
            "[Epoch 8/200 Batch 60/97] Loss=0.36389.\n",
            "[Epoch 8/200 Batch 70/97] Loss=0.40748.\n",
            "[Epoch 8/200 Batch 80/97] Loss=0.38372.\n",
            "[Epoch 8/200 Batch 90/97] Loss=0.39748.\n",
            "[Epoch 9/200 Batch 10/97] Loss=0.39124.\n",
            "[Epoch 9/200 Batch 20/97] Loss=0.35782.\n",
            "[Epoch 9/200 Batch 30/97] Loss=0.37877.\n",
            "[Epoch 9/200 Batch 40/97] Loss=0.38437.\n",
            "[Epoch 9/200 Batch 50/97] Loss=0.40074.\n",
            "[Epoch 9/200 Batch 60/97] Loss=0.42797.\n",
            "[Epoch 9/200 Batch 70/97] Loss=0.42030.\n",
            "[Epoch 9/200 Batch 80/97] Loss=0.43730.\n",
            "[Epoch 9/200 Batch 90/97] Loss=0.39901.\n",
            "[Epoch 10/200 Batch 10/97] Loss=0.44823.\n",
            "[Epoch 10/200 Batch 20/97] Loss=0.42404.\n",
            "[Epoch 10/200 Batch 30/97] Loss=0.37794.\n",
            "[Epoch 10/200 Batch 40/97] Loss=0.41741.\n",
            "[Epoch 10/200 Batch 50/97] Loss=0.44162.\n",
            "[Epoch 10/200 Batch 60/97] Loss=0.42153.\n",
            "[Epoch 10/200 Batch 70/97] Loss=0.41644.\n",
            "[Epoch 10/200 Batch 80/97] Loss=0.42133.\n",
            "[Epoch 10/200 Batch 90/97] Loss=0.43520.\n",
            "[Epoch 11/200 Batch 10/97] Loss=0.39394.\n",
            "[Epoch 11/200 Batch 20/97] Loss=0.40350.\n",
            "[Epoch 11/200 Batch 30/97] Loss=0.39790.\n",
            "[Epoch 11/200 Batch 40/97] Loss=0.40705.\n",
            "[Epoch 11/200 Batch 50/97] Loss=0.39164.\n",
            "[Epoch 11/200 Batch 60/97] Loss=0.36904.\n",
            "[Epoch 11/200 Batch 70/97] Loss=0.41086.\n",
            "[Epoch 11/200 Batch 80/97] Loss=0.44732.\n",
            "[Epoch 11/200 Batch 90/97] Loss=0.42597.\n",
            "[Epoch 12/200 Batch 10/97] Loss=0.41832.\n",
            "[Epoch 12/200 Batch 20/97] Loss=0.42256.\n",
            "[Epoch 12/200 Batch 30/97] Loss=0.42184.\n",
            "[Epoch 12/200 Batch 40/97] Loss=0.45852.\n",
            "[Epoch 12/200 Batch 50/97] Loss=0.43455.\n",
            "[Epoch 12/200 Batch 60/97] Loss=0.42187.\n",
            "[Epoch 12/200 Batch 70/97] Loss=0.41426.\n",
            "[Epoch 12/200 Batch 80/97] Loss=0.41233.\n",
            "[Epoch 12/200 Batch 90/97] Loss=0.45066.\n",
            "[Epoch 13/200 Batch 10/97] Loss=0.41672.\n",
            "[Epoch 13/200 Batch 20/97] Loss=0.44815.\n",
            "[Epoch 13/200 Batch 30/97] Loss=0.41630.\n",
            "[Epoch 13/200 Batch 40/97] Loss=0.42316.\n",
            "[Epoch 13/200 Batch 50/97] Loss=0.44146.\n",
            "[Epoch 13/200 Batch 60/97] Loss=0.42747.\n",
            "[Epoch 13/200 Batch 70/97] Loss=0.46077.\n",
            "[Epoch 13/200 Batch 80/97] Loss=0.42795.\n",
            "[Epoch 13/200 Batch 90/97] Loss=0.44200.\n",
            "[Epoch 14/200 Batch 10/97] Loss=0.39798.\n",
            "[Epoch 14/200 Batch 20/97] Loss=0.46356.\n",
            "[Epoch 14/200 Batch 30/97] Loss=0.46670.\n",
            "[Epoch 14/200 Batch 40/97] Loss=0.43155.\n",
            "[Epoch 14/200 Batch 50/97] Loss=0.43596.\n",
            "[Epoch 14/200 Batch 60/97] Loss=0.44782.\n",
            "[Epoch 14/200 Batch 70/97] Loss=0.42094.\n",
            "[Epoch 14/200 Batch 80/97] Loss=0.42987.\n",
            "[Epoch 14/200 Batch 90/97] Loss=0.44813.\n",
            "[Epoch 15/200 Batch 10/97] Loss=0.41980.\n",
            "[Epoch 15/200 Batch 20/97] Loss=0.44771.\n",
            "[Epoch 15/200 Batch 30/97] Loss=0.42052.\n",
            "[Epoch 15/200 Batch 40/97] Loss=0.45973.\n",
            "[Epoch 15/200 Batch 50/97] Loss=0.44686.\n",
            "[Epoch 15/200 Batch 60/97] Loss=0.40926.\n",
            "[Epoch 15/200 Batch 70/97] Loss=0.39786.\n",
            "[Epoch 15/200 Batch 80/97] Loss=0.40801.\n",
            "[Epoch 15/200 Batch 90/97] Loss=0.42554.\n",
            "[Epoch 16/200 Batch 10/97] Loss=0.46133.\n",
            "[Epoch 16/200 Batch 20/97] Loss=0.41384.\n",
            "[Epoch 16/200 Batch 30/97] Loss=0.40983.\n",
            "[Epoch 16/200 Batch 40/97] Loss=0.43421.\n",
            "[Epoch 16/200 Batch 50/97] Loss=0.39764.\n",
            "[Epoch 16/200 Batch 60/97] Loss=0.42714.\n",
            "[Epoch 16/200 Batch 70/97] Loss=0.42319.\n",
            "[Epoch 16/200 Batch 80/97] Loss=0.42478.\n",
            "[Epoch 16/200 Batch 90/97] Loss=0.41466.\n",
            "[Epoch 17/200 Batch 10/97] Loss=0.41552.\n",
            "[Epoch 17/200 Batch 20/97] Loss=0.41609.\n",
            "[Epoch 17/200 Batch 30/97] Loss=0.42167.\n",
            "[Epoch 17/200 Batch 40/97] Loss=0.41594.\n",
            "[Epoch 17/200 Batch 50/97] Loss=0.39679.\n",
            "[Epoch 17/200 Batch 60/97] Loss=0.42911.\n",
            "[Epoch 17/200 Batch 70/97] Loss=0.42843.\n",
            "[Epoch 17/200 Batch 80/97] Loss=0.44957.\n",
            "[Epoch 17/200 Batch 90/97] Loss=0.45822.\n",
            "[Epoch 18/200 Batch 10/97] Loss=0.44520.\n",
            "[Epoch 18/200 Batch 20/97] Loss=0.38128.\n",
            "[Epoch 18/200 Batch 30/97] Loss=0.41567.\n",
            "[Epoch 18/200 Batch 40/97] Loss=0.42578.\n",
            "[Epoch 18/200 Batch 50/97] Loss=0.42061.\n",
            "[Epoch 18/200 Batch 60/97] Loss=0.40841.\n",
            "[Epoch 18/200 Batch 70/97] Loss=0.46286.\n",
            "[Epoch 18/200 Batch 80/97] Loss=0.38675.\n",
            "[Epoch 18/200 Batch 90/97] Loss=0.45420.\n",
            "[Epoch 19/200 Batch 10/97] Loss=0.44337.\n",
            "[Epoch 19/200 Batch 20/97] Loss=0.43583.\n",
            "[Epoch 19/200 Batch 30/97] Loss=0.41960.\n",
            "[Epoch 19/200 Batch 40/97] Loss=0.43697.\n",
            "[Epoch 19/200 Batch 50/97] Loss=0.40887.\n",
            "[Epoch 19/200 Batch 60/97] Loss=0.42702.\n",
            "[Epoch 19/200 Batch 70/97] Loss=0.44961.\n",
            "[Epoch 19/200 Batch 80/97] Loss=0.43526.\n",
            "[Epoch 19/200 Batch 90/97] Loss=0.42932.\n",
            "[Epoch 20/200 Batch 10/97] Loss=0.41468.\n",
            "[Epoch 20/200 Batch 20/97] Loss=0.44596.\n",
            "[Epoch 20/200 Batch 30/97] Loss=0.44484.\n",
            "[Epoch 20/200 Batch 40/97] Loss=0.45681.\n",
            "[Epoch 20/200 Batch 50/97] Loss=0.45206.\n",
            "[Epoch 20/200 Batch 60/97] Loss=0.43927.\n",
            "[Epoch 20/200 Batch 70/97] Loss=0.42116.\n",
            "[Epoch 20/200 Batch 80/97] Loss=0.44210.\n",
            "[Epoch 20/200 Batch 90/97] Loss=0.44457.\n",
            "[Epoch 21/200 Batch 10/97] Loss=0.39696.\n",
            "[Epoch 21/200 Batch 20/97] Loss=0.46047.\n",
            "[Epoch 21/200 Batch 30/97] Loss=0.43395.\n",
            "[Epoch 21/200 Batch 40/97] Loss=0.43475.\n",
            "[Epoch 21/200 Batch 50/97] Loss=0.42227.\n",
            "[Epoch 21/200 Batch 60/97] Loss=0.44955.\n",
            "[Epoch 21/200 Batch 70/97] Loss=0.41503.\n",
            "[Epoch 21/200 Batch 80/97] Loss=0.43906.\n",
            "[Epoch 21/200 Batch 90/97] Loss=0.42178.\n",
            "[Epoch 22/200 Batch 10/97] Loss=0.43286.\n",
            "[Epoch 22/200 Batch 20/97] Loss=0.43015.\n",
            "[Epoch 22/200 Batch 30/97] Loss=0.43592.\n",
            "[Epoch 22/200 Batch 40/97] Loss=0.41649.\n",
            "[Epoch 22/200 Batch 50/97] Loss=0.42509.\n",
            "[Epoch 22/200 Batch 60/97] Loss=0.43399.\n",
            "[Epoch 22/200 Batch 70/97] Loss=0.45280.\n",
            "[Epoch 22/200 Batch 80/97] Loss=0.43554.\n",
            "[Epoch 22/200 Batch 90/97] Loss=0.42298.\n",
            "[Epoch 23/200 Batch 10/97] Loss=0.43642.\n",
            "[Epoch 23/200 Batch 20/97] Loss=0.45224.\n",
            "[Epoch 23/200 Batch 30/97] Loss=0.44147.\n",
            "[Epoch 23/200 Batch 40/97] Loss=0.41625.\n",
            "[Epoch 23/200 Batch 50/97] Loss=0.41015.\n",
            "[Epoch 23/200 Batch 60/97] Loss=0.46970.\n",
            "[Epoch 23/200 Batch 70/97] Loss=0.39954.\n",
            "[Epoch 23/200 Batch 80/97] Loss=0.44186.\n",
            "[Epoch 23/200 Batch 90/97] Loss=0.43183.\n",
            "[Epoch 24/200 Batch 10/97] Loss=0.41847.\n",
            "[Epoch 24/200 Batch 20/97] Loss=0.43437.\n",
            "[Epoch 24/200 Batch 30/97] Loss=0.40530.\n",
            "[Epoch 24/200 Batch 40/97] Loss=0.41344.\n",
            "[Epoch 24/200 Batch 50/97] Loss=0.42890.\n",
            "[Epoch 24/200 Batch 60/97] Loss=0.38540.\n",
            "[Epoch 24/200 Batch 70/97] Loss=0.38815.\n",
            "[Epoch 24/200 Batch 80/97] Loss=0.40760.\n",
            "[Epoch 24/200 Batch 90/97] Loss=0.41145.\n",
            "[Epoch 25/200 Batch 10/97] Loss=0.42225.\n",
            "[Epoch 25/200 Batch 20/97] Loss=0.42250.\n",
            "[Epoch 25/200 Batch 30/97] Loss=0.40155.\n",
            "[Epoch 25/200 Batch 40/97] Loss=0.40149.\n",
            "[Epoch 25/200 Batch 50/97] Loss=0.41413.\n",
            "[Epoch 25/200 Batch 60/97] Loss=0.41442.\n",
            "[Epoch 25/200 Batch 70/97] Loss=0.41326.\n",
            "[Epoch 25/200 Batch 80/97] Loss=0.40053.\n",
            "[Epoch 25/200 Batch 90/97] Loss=0.39555.\n",
            "[Epoch 26/200 Batch 10/97] Loss=0.39623.\n",
            "[Epoch 26/200 Batch 20/97] Loss=0.41719.\n",
            "[Epoch 26/200 Batch 30/97] Loss=0.42563.\n",
            "[Epoch 26/200 Batch 40/97] Loss=0.40363.\n",
            "[Epoch 26/200 Batch 50/97] Loss=0.42793.\n",
            "[Epoch 26/200 Batch 60/97] Loss=0.41013.\n",
            "[Epoch 26/200 Batch 70/97] Loss=0.41623.\n",
            "[Epoch 26/200 Batch 80/97] Loss=0.41021.\n",
            "[Epoch 26/200 Batch 90/97] Loss=0.42197.\n",
            "[Epoch 27/200 Batch 10/97] Loss=0.41392.\n",
            "[Epoch 27/200 Batch 20/97] Loss=0.41862.\n",
            "[Epoch 27/200 Batch 30/97] Loss=0.39534.\n",
            "[Epoch 27/200 Batch 40/97] Loss=0.39611.\n",
            "[Epoch 27/200 Batch 50/97] Loss=0.36843.\n",
            "[Epoch 27/200 Batch 60/97] Loss=0.38553.\n",
            "[Epoch 27/200 Batch 70/97] Loss=0.43321.\n",
            "[Epoch 27/200 Batch 80/97] Loss=0.40628.\n",
            "[Epoch 27/200 Batch 90/97] Loss=0.40263.\n",
            "[Epoch 28/200 Batch 10/97] Loss=0.41119.\n",
            "[Epoch 28/200 Batch 20/97] Loss=0.40525.\n",
            "[Epoch 28/200 Batch 30/97] Loss=0.41591.\n",
            "[Epoch 28/200 Batch 40/97] Loss=0.40785.\n",
            "[Epoch 28/200 Batch 50/97] Loss=0.42544.\n",
            "[Epoch 28/200 Batch 60/97] Loss=0.39237.\n",
            "[Epoch 28/200 Batch 70/97] Loss=0.37417.\n",
            "[Epoch 28/200 Batch 80/97] Loss=0.40446.\n",
            "[Epoch 28/200 Batch 90/97] Loss=0.41387.\n",
            "[Epoch 29/200 Batch 10/97] Loss=0.40073.\n",
            "[Epoch 29/200 Batch 20/97] Loss=0.38251.\n",
            "[Epoch 29/200 Batch 30/97] Loss=0.38264.\n",
            "[Epoch 29/200 Batch 40/97] Loss=0.38732.\n",
            "[Epoch 29/200 Batch 50/97] Loss=0.40243.\n",
            "[Epoch 29/200 Batch 60/97] Loss=0.40668.\n",
            "[Epoch 29/200 Batch 70/97] Loss=0.37476.\n",
            "[Epoch 29/200 Batch 80/97] Loss=0.38562.\n",
            "[Epoch 29/200 Batch 90/97] Loss=0.38575.\n",
            "[Epoch 30/200 Batch 10/97] Loss=0.38270.\n",
            "[Epoch 30/200 Batch 20/97] Loss=0.38033.\n",
            "[Epoch 30/200 Batch 30/97] Loss=0.37974.\n",
            "[Epoch 30/200 Batch 40/97] Loss=0.39024.\n",
            "[Epoch 30/200 Batch 50/97] Loss=0.35148.\n",
            "[Epoch 30/200 Batch 60/97] Loss=0.40606.\n",
            "[Epoch 30/200 Batch 70/97] Loss=0.37177.\n",
            "[Epoch 30/200 Batch 80/97] Loss=0.39093.\n",
            "[Epoch 30/200 Batch 90/97] Loss=0.40443.\n",
            "[Epoch 31/200 Batch 10/97] Loss=0.39827.\n",
            "[Epoch 31/200 Batch 20/97] Loss=0.39573.\n",
            "[Epoch 31/200 Batch 30/97] Loss=0.37850.\n",
            "[Epoch 31/200 Batch 40/97] Loss=0.35759.\n",
            "[Epoch 31/200 Batch 50/97] Loss=0.36659.\n",
            "[Epoch 31/200 Batch 60/97] Loss=0.35869.\n",
            "[Epoch 31/200 Batch 70/97] Loss=0.39932.\n",
            "[Epoch 31/200 Batch 80/97] Loss=0.38099.\n",
            "[Epoch 31/200 Batch 90/97] Loss=0.36766.\n",
            "[Epoch 32/200 Batch 10/97] Loss=0.38430.\n",
            "[Epoch 32/200 Batch 20/97] Loss=0.38956.\n",
            "[Epoch 32/200 Batch 30/97] Loss=0.36700.\n",
            "[Epoch 32/200 Batch 40/97] Loss=0.38711.\n",
            "[Epoch 32/200 Batch 50/97] Loss=0.38590.\n",
            "[Epoch 32/200 Batch 60/97] Loss=0.38669.\n",
            "[Epoch 32/200 Batch 70/97] Loss=0.39630.\n",
            "[Epoch 32/200 Batch 80/97] Loss=0.34833.\n",
            "[Epoch 32/200 Batch 90/97] Loss=0.39300.\n",
            "[Epoch 33/200 Batch 10/97] Loss=0.37456.\n",
            "[Epoch 33/200 Batch 20/97] Loss=0.37971.\n",
            "[Epoch 33/200 Batch 30/97] Loss=0.37077.\n",
            "[Epoch 33/200 Batch 40/97] Loss=0.36141.\n",
            "[Epoch 33/200 Batch 50/97] Loss=0.37541.\n",
            "[Epoch 33/200 Batch 60/97] Loss=0.37420.\n",
            "[Epoch 33/200 Batch 70/97] Loss=0.37850.\n",
            "[Epoch 33/200 Batch 80/97] Loss=0.36247.\n",
            "[Epoch 33/200 Batch 90/97] Loss=0.36278.\n",
            "[Epoch 34/200 Batch 10/97] Loss=0.37213.\n",
            "[Epoch 34/200 Batch 20/97] Loss=0.38667.\n",
            "[Epoch 34/200 Batch 30/97] Loss=0.36710.\n",
            "[Epoch 34/200 Batch 40/97] Loss=0.38416.\n",
            "[Epoch 34/200 Batch 50/97] Loss=0.37466.\n",
            "[Epoch 34/200 Batch 60/97] Loss=0.35272.\n",
            "[Epoch 34/200 Batch 70/97] Loss=0.36793.\n",
            "[Epoch 34/200 Batch 80/97] Loss=0.36902.\n",
            "[Epoch 34/200 Batch 90/97] Loss=0.36707.\n",
            "[Epoch 35/200 Batch 10/97] Loss=0.38732.\n",
            "[Epoch 35/200 Batch 20/97] Loss=0.36751.\n",
            "[Epoch 35/200 Batch 30/97] Loss=0.38602.\n",
            "[Epoch 35/200 Batch 40/97] Loss=0.35503.\n",
            "[Epoch 35/200 Batch 50/97] Loss=0.39708.\n",
            "[Epoch 35/200 Batch 60/97] Loss=0.38242.\n",
            "[Epoch 35/200 Batch 70/97] Loss=0.38081.\n",
            "[Epoch 35/200 Batch 80/97] Loss=0.40250.\n",
            "[Epoch 35/200 Batch 90/97] Loss=0.39758.\n",
            "[Epoch 36/200 Batch 10/97] Loss=0.37150.\n",
            "[Epoch 36/200 Batch 20/97] Loss=0.37177.\n",
            "[Epoch 36/200 Batch 30/97] Loss=0.38355.\n",
            "[Epoch 36/200 Batch 40/97] Loss=0.33680.\n",
            "[Epoch 36/200 Batch 50/97] Loss=0.36338.\n",
            "[Epoch 36/200 Batch 60/97] Loss=0.34258.\n",
            "[Epoch 36/200 Batch 70/97] Loss=0.33010.\n",
            "[Epoch 36/200 Batch 80/97] Loss=0.35906.\n",
            "[Epoch 36/200 Batch 90/97] Loss=0.35477.\n",
            "[Epoch 37/200 Batch 10/97] Loss=0.33846.\n",
            "[Epoch 37/200 Batch 20/97] Loss=0.36678.\n",
            "[Epoch 37/200 Batch 30/97] Loss=0.36722.\n",
            "[Epoch 37/200 Batch 40/97] Loss=0.36523.\n",
            "[Epoch 37/200 Batch 50/97] Loss=0.36684.\n",
            "[Epoch 37/200 Batch 60/97] Loss=0.37069.\n",
            "[Epoch 37/200 Batch 70/97] Loss=0.34549.\n",
            "[Epoch 37/200 Batch 80/97] Loss=0.37187.\n",
            "[Epoch 37/200 Batch 90/97] Loss=0.35447.\n",
            "[Epoch 38/200 Batch 10/97] Loss=0.40126.\n",
            "[Epoch 38/200 Batch 20/97] Loss=0.34734.\n",
            "[Epoch 38/200 Batch 30/97] Loss=0.34249.\n",
            "[Epoch 38/200 Batch 40/97] Loss=0.34139.\n",
            "[Epoch 38/200 Batch 50/97] Loss=0.35189.\n",
            "[Epoch 38/200 Batch 60/97] Loss=0.35610.\n",
            "[Epoch 38/200 Batch 70/97] Loss=0.36238.\n",
            "[Epoch 38/200 Batch 80/97] Loss=0.34617.\n",
            "[Epoch 38/200 Batch 90/97] Loss=0.34989.\n",
            "[Epoch 39/200 Batch 10/97] Loss=0.35878.\n",
            "[Epoch 39/200 Batch 20/97] Loss=0.35398.\n",
            "[Epoch 39/200 Batch 30/97] Loss=0.36783.\n",
            "[Epoch 39/200 Batch 40/97] Loss=0.34685.\n",
            "[Epoch 39/200 Batch 50/97] Loss=0.33787.\n",
            "[Epoch 39/200 Batch 60/97] Loss=0.34940.\n",
            "[Epoch 39/200 Batch 70/97] Loss=0.32804.\n",
            "[Epoch 39/200 Batch 80/97] Loss=0.35325.\n",
            "[Epoch 39/200 Batch 90/97] Loss=0.37473.\n",
            "[Epoch 40/200 Batch 10/97] Loss=0.36028.\n",
            "[Epoch 40/200 Batch 20/97] Loss=0.35446.\n",
            "[Epoch 40/200 Batch 30/97] Loss=0.35305.\n",
            "[Epoch 40/200 Batch 40/97] Loss=0.34261.\n",
            "[Epoch 40/200 Batch 50/97] Loss=0.35251.\n",
            "[Epoch 40/200 Batch 60/97] Loss=0.36501.\n",
            "[Epoch 40/200 Batch 70/97] Loss=0.35990.\n",
            "[Epoch 40/200 Batch 80/97] Loss=0.35763.\n",
            "[Epoch 40/200 Batch 90/97] Loss=0.39487.\n",
            "[Epoch 41/200 Batch 10/97] Loss=0.36398.\n",
            "[Epoch 41/200 Batch 20/97] Loss=0.34150.\n",
            "[Epoch 41/200 Batch 30/97] Loss=0.35889.\n",
            "[Epoch 41/200 Batch 40/97] Loss=0.36447.\n",
            "[Epoch 41/200 Batch 50/97] Loss=0.35397.\n",
            "[Epoch 41/200 Batch 60/97] Loss=0.34591.\n",
            "[Epoch 41/200 Batch 70/97] Loss=0.35790.\n",
            "[Epoch 41/200 Batch 80/97] Loss=0.35691.\n",
            "[Epoch 41/200 Batch 90/97] Loss=0.34620.\n",
            "[Epoch 42/200 Batch 10/97] Loss=0.36767.\n",
            "[Epoch 42/200 Batch 20/97] Loss=0.34892.\n",
            "[Epoch 42/200 Batch 30/97] Loss=0.35928.\n",
            "[Epoch 42/200 Batch 40/97] Loss=0.33727.\n",
            "[Epoch 42/200 Batch 50/97] Loss=0.34958.\n",
            "[Epoch 42/200 Batch 60/97] Loss=0.34946.\n",
            "[Epoch 42/200 Batch 70/97] Loss=0.33148.\n",
            "[Epoch 42/200 Batch 80/97] Loss=0.34931.\n",
            "[Epoch 42/200 Batch 90/97] Loss=0.34039.\n",
            "[Epoch 43/200 Batch 10/97] Loss=0.33449.\n",
            "[Epoch 43/200 Batch 20/97] Loss=0.35680.\n",
            "[Epoch 43/200 Batch 30/97] Loss=0.33488.\n",
            "[Epoch 43/200 Batch 40/97] Loss=0.35470.\n",
            "[Epoch 43/200 Batch 50/97] Loss=0.35785.\n",
            "[Epoch 43/200 Batch 60/97] Loss=0.31416.\n",
            "[Epoch 43/200 Batch 70/97] Loss=0.34254.\n",
            "[Epoch 43/200 Batch 80/97] Loss=0.32071.\n",
            "[Epoch 43/200 Batch 90/97] Loss=0.33838.\n",
            "[Epoch 44/200 Batch 10/97] Loss=0.33958.\n",
            "[Epoch 44/200 Batch 20/97] Loss=0.34937.\n",
            "[Epoch 44/200 Batch 30/97] Loss=0.35134.\n",
            "[Epoch 44/200 Batch 40/97] Loss=0.34963.\n",
            "[Epoch 44/200 Batch 50/97] Loss=0.33875.\n",
            "[Epoch 44/200 Batch 60/97] Loss=0.33046.\n",
            "[Epoch 44/200 Batch 70/97] Loss=0.37194.\n",
            "[Epoch 44/200 Batch 80/97] Loss=0.33320.\n",
            "[Epoch 44/200 Batch 90/97] Loss=0.34041.\n",
            "[Epoch 45/200 Batch 10/97] Loss=0.34580.\n",
            "[Epoch 45/200 Batch 20/97] Loss=0.32777.\n",
            "[Epoch 45/200 Batch 30/97] Loss=0.33506.\n",
            "[Epoch 45/200 Batch 40/97] Loss=0.32031.\n",
            "[Epoch 45/200 Batch 50/97] Loss=0.33084.\n",
            "[Epoch 45/200 Batch 60/97] Loss=0.35151.\n",
            "[Epoch 45/200 Batch 70/97] Loss=0.32890.\n",
            "[Epoch 45/200 Batch 80/97] Loss=0.34233.\n",
            "[Epoch 45/200 Batch 90/97] Loss=0.33467.\n",
            "[Epoch 46/200 Batch 10/97] Loss=0.38918.\n",
            "[Epoch 46/200 Batch 20/97] Loss=0.33563.\n",
            "[Epoch 46/200 Batch 30/97] Loss=0.34917.\n",
            "[Epoch 46/200 Batch 40/97] Loss=0.34933.\n",
            "[Epoch 46/200 Batch 50/97] Loss=0.32519.\n",
            "[Epoch 46/200 Batch 60/97] Loss=0.33404.\n",
            "[Epoch 46/200 Batch 70/97] Loss=0.35173.\n",
            "[Epoch 46/200 Batch 80/97] Loss=0.33475.\n",
            "[Epoch 46/200 Batch 90/97] Loss=0.32485.\n",
            "[Epoch 47/200 Batch 10/97] Loss=0.34086.\n",
            "[Epoch 47/200 Batch 20/97] Loss=0.33302.\n",
            "[Epoch 47/200 Batch 30/97] Loss=0.35827.\n",
            "[Epoch 47/200 Batch 40/97] Loss=0.34075.\n",
            "[Epoch 47/200 Batch 50/97] Loss=0.31868.\n",
            "[Epoch 47/200 Batch 60/97] Loss=0.33717.\n",
            "[Epoch 47/200 Batch 70/97] Loss=0.32379.\n",
            "[Epoch 47/200 Batch 80/97] Loss=0.34639.\n",
            "[Epoch 47/200 Batch 90/97] Loss=0.35375.\n",
            "[Epoch 48/200 Batch 10/97] Loss=0.33634.\n",
            "[Epoch 48/200 Batch 20/97] Loss=0.33731.\n",
            "[Epoch 48/200 Batch 30/97] Loss=0.34985.\n",
            "[Epoch 48/200 Batch 40/97] Loss=0.33624.\n",
            "[Epoch 48/200 Batch 50/97] Loss=0.36500.\n",
            "[Epoch 48/200 Batch 60/97] Loss=0.33987.\n",
            "[Epoch 48/200 Batch 70/97] Loss=0.35795.\n",
            "[Epoch 48/200 Batch 80/97] Loss=0.32940.\n",
            "[Epoch 48/200 Batch 90/97] Loss=0.34089.\n",
            "[Epoch 49/200 Batch 10/97] Loss=0.35261.\n",
            "[Epoch 49/200 Batch 20/97] Loss=0.34197.\n",
            "[Epoch 49/200 Batch 30/97] Loss=0.35372.\n",
            "[Epoch 49/200 Batch 40/97] Loss=0.34718.\n",
            "[Epoch 49/200 Batch 50/97] Loss=0.32464.\n",
            "[Epoch 49/200 Batch 60/97] Loss=0.33105.\n",
            "[Epoch 49/200 Batch 70/97] Loss=0.35321.\n",
            "[Epoch 49/200 Batch 80/97] Loss=0.35419.\n",
            "[Epoch 49/200 Batch 90/97] Loss=0.34460.\n",
            "[Epoch 50/200 Batch 10/97] Loss=0.32051.\n",
            "[Epoch 50/200 Batch 20/97] Loss=0.33914.\n",
            "[Epoch 50/200 Batch 30/97] Loss=0.33893.\n",
            "[Epoch 50/200 Batch 40/97] Loss=0.34586.\n",
            "[Epoch 50/200 Batch 50/97] Loss=0.33085.\n",
            "[Epoch 50/200 Batch 60/97] Loss=0.32029.\n",
            "[Epoch 50/200 Batch 70/97] Loss=0.34400.\n",
            "[Epoch 50/200 Batch 80/97] Loss=0.32359.\n",
            "[Epoch 50/200 Batch 90/97] Loss=0.35313.\n",
            "[Epoch 51/200 Batch 10/97] Loss=0.35515.\n",
            "[Epoch 51/200 Batch 20/97] Loss=0.31169.\n",
            "[Epoch 51/200 Batch 30/97] Loss=0.32176.\n",
            "[Epoch 51/200 Batch 40/97] Loss=0.33877.\n",
            "[Epoch 51/200 Batch 50/97] Loss=0.35060.\n",
            "[Epoch 51/200 Batch 60/97] Loss=0.34177.\n",
            "[Epoch 51/200 Batch 70/97] Loss=0.34669.\n",
            "[Epoch 51/200 Batch 80/97] Loss=0.35130.\n",
            "[Epoch 51/200 Batch 90/97] Loss=0.32639.\n",
            "[Epoch 52/200 Batch 10/97] Loss=0.33881.\n",
            "[Epoch 52/200 Batch 20/97] Loss=0.32944.\n",
            "[Epoch 52/200 Batch 30/97] Loss=0.33543.\n",
            "[Epoch 52/200 Batch 40/97] Loss=0.33388.\n",
            "[Epoch 52/200 Batch 50/97] Loss=0.33222.\n",
            "[Epoch 52/200 Batch 60/97] Loss=0.34392.\n",
            "[Epoch 52/200 Batch 70/97] Loss=0.33548.\n",
            "[Epoch 52/200 Batch 80/97] Loss=0.34902.\n",
            "[Epoch 52/200 Batch 90/97] Loss=0.36610.\n",
            "[Epoch 53/200 Batch 10/97] Loss=0.34172.\n",
            "[Epoch 53/200 Batch 20/97] Loss=0.32435.\n",
            "[Epoch 53/200 Batch 30/97] Loss=0.33407.\n",
            "[Epoch 53/200 Batch 40/97] Loss=0.33675.\n",
            "[Epoch 53/200 Batch 50/97] Loss=0.34787.\n",
            "[Epoch 53/200 Batch 60/97] Loss=0.35615.\n",
            "[Epoch 53/200 Batch 70/97] Loss=0.29802.\n",
            "[Epoch 53/200 Batch 80/97] Loss=0.33961.\n",
            "[Epoch 53/200 Batch 90/97] Loss=0.32168.\n",
            "[Epoch 54/200 Batch 10/97] Loss=0.30462.\n",
            "[Epoch 54/200 Batch 20/97] Loss=0.35700.\n",
            "[Epoch 54/200 Batch 30/97] Loss=0.34448.\n",
            "[Epoch 54/200 Batch 40/97] Loss=0.33884.\n",
            "[Epoch 54/200 Batch 50/97] Loss=0.33984.\n",
            "[Epoch 54/200 Batch 60/97] Loss=0.32884.\n",
            "[Epoch 54/200 Batch 70/97] Loss=0.32863.\n",
            "[Epoch 54/200 Batch 80/97] Loss=0.34978.\n",
            "[Epoch 54/200 Batch 90/97] Loss=0.35191.\n",
            "[Epoch 55/200 Batch 10/97] Loss=0.34669.\n",
            "[Epoch 55/200 Batch 20/97] Loss=0.31239.\n",
            "[Epoch 55/200 Batch 30/97] Loss=0.33679.\n",
            "[Epoch 55/200 Batch 40/97] Loss=0.34587.\n",
            "[Epoch 55/200 Batch 50/97] Loss=0.34190.\n",
            "[Epoch 55/200 Batch 60/97] Loss=0.35175.\n",
            "[Epoch 55/200 Batch 70/97] Loss=0.35431.\n",
            "[Epoch 55/200 Batch 80/97] Loss=0.33802.\n",
            "[Epoch 55/200 Batch 90/97] Loss=0.33102.\n",
            "[Epoch 56/200 Batch 10/97] Loss=0.34520.\n",
            "[Epoch 56/200 Batch 20/97] Loss=0.34427.\n",
            "[Epoch 56/200 Batch 30/97] Loss=0.36327.\n",
            "[Epoch 56/200 Batch 40/97] Loss=0.32399.\n",
            "[Epoch 56/200 Batch 50/97] Loss=0.34133.\n",
            "[Epoch 56/200 Batch 60/97] Loss=0.32019.\n",
            "[Epoch 56/200 Batch 70/97] Loss=0.33735.\n",
            "[Epoch 56/200 Batch 80/97] Loss=0.33494.\n",
            "[Epoch 56/200 Batch 90/97] Loss=0.34086.\n",
            "[Epoch 57/200 Batch 10/97] Loss=0.34487.\n",
            "[Epoch 57/200 Batch 20/97] Loss=0.34609.\n",
            "[Epoch 57/200 Batch 30/97] Loss=0.33850.\n",
            "[Epoch 57/200 Batch 40/97] Loss=0.31693.\n",
            "[Epoch 57/200 Batch 50/97] Loss=0.32408.\n",
            "[Epoch 57/200 Batch 60/97] Loss=0.33990.\n",
            "[Epoch 57/200 Batch 70/97] Loss=0.33847.\n",
            "[Epoch 57/200 Batch 80/97] Loss=0.34201.\n",
            "[Epoch 57/200 Batch 90/97] Loss=0.34740.\n",
            "[Epoch 58/200 Batch 10/97] Loss=0.32360.\n",
            "[Epoch 58/200 Batch 20/97] Loss=0.34544.\n",
            "[Epoch 58/200 Batch 30/97] Loss=0.32470.\n",
            "[Epoch 58/200 Batch 40/97] Loss=0.33930.\n",
            "[Epoch 58/200 Batch 50/97] Loss=0.33476.\n",
            "[Epoch 58/200 Batch 60/97] Loss=0.34635.\n",
            "[Epoch 58/200 Batch 70/97] Loss=0.33863.\n",
            "[Epoch 58/200 Batch 80/97] Loss=0.34338.\n",
            "[Epoch 58/200 Batch 90/97] Loss=0.33846.\n",
            "[Epoch 59/200 Batch 10/97] Loss=0.32485.\n",
            "[Epoch 59/200 Batch 20/97] Loss=0.34159.\n",
            "[Epoch 59/200 Batch 30/97] Loss=0.35077.\n",
            "[Epoch 59/200 Batch 40/97] Loss=0.32879.\n",
            "[Epoch 59/200 Batch 50/97] Loss=0.34781.\n",
            "[Epoch 59/200 Batch 60/97] Loss=0.33661.\n",
            "[Epoch 59/200 Batch 70/97] Loss=0.34998.\n",
            "[Epoch 59/200 Batch 80/97] Loss=0.34404.\n",
            "[Epoch 59/200 Batch 90/97] Loss=0.34379.\n",
            "[Epoch 60/200 Batch 10/97] Loss=0.33546.\n",
            "[Epoch 60/200 Batch 20/97] Loss=0.34602.\n",
            "[Epoch 60/200 Batch 30/97] Loss=0.35533.\n",
            "[Epoch 60/200 Batch 40/97] Loss=0.36763.\n",
            "[Epoch 60/200 Batch 50/97] Loss=0.33449.\n",
            "[Epoch 60/200 Batch 60/97] Loss=0.34976.\n",
            "[Epoch 60/200 Batch 70/97] Loss=0.36456.\n",
            "[Epoch 60/200 Batch 80/97] Loss=0.32994.\n",
            "[Epoch 60/200 Batch 90/97] Loss=0.33890.\n",
            "[Epoch 61/200 Batch 10/97] Loss=0.34340.\n",
            "[Epoch 61/200 Batch 20/97] Loss=0.35042.\n",
            "[Epoch 61/200 Batch 30/97] Loss=0.32881.\n",
            "[Epoch 61/200 Batch 40/97] Loss=0.34168.\n",
            "[Epoch 61/200 Batch 50/97] Loss=0.35459.\n",
            "[Epoch 61/200 Batch 60/97] Loss=0.35758.\n",
            "[Epoch 61/200 Batch 70/97] Loss=0.33519.\n",
            "[Epoch 61/200 Batch 80/97] Loss=0.32437.\n",
            "[Epoch 61/200 Batch 90/97] Loss=0.34531.\n",
            "[Epoch 62/200 Batch 10/97] Loss=0.35763.\n",
            "[Epoch 62/200 Batch 20/97] Loss=0.32269.\n",
            "[Epoch 62/200 Batch 30/97] Loss=0.33149.\n",
            "[Epoch 62/200 Batch 40/97] Loss=0.34665.\n",
            "[Epoch 62/200 Batch 50/97] Loss=0.33426.\n",
            "[Epoch 62/200 Batch 60/97] Loss=0.33205.\n",
            "[Epoch 62/200 Batch 70/97] Loss=0.35252.\n",
            "[Epoch 62/200 Batch 80/97] Loss=0.34913.\n",
            "[Epoch 62/200 Batch 90/97] Loss=0.34778.\n",
            "[Epoch 63/200 Batch 10/97] Loss=0.34338.\n",
            "[Epoch 63/200 Batch 20/97] Loss=0.35405.\n",
            "[Epoch 63/200 Batch 30/97] Loss=0.33041.\n",
            "[Epoch 63/200 Batch 40/97] Loss=0.32635.\n",
            "[Epoch 63/200 Batch 50/97] Loss=0.32725.\n",
            "[Epoch 63/200 Batch 60/97] Loss=0.35787.\n",
            "[Epoch 63/200 Batch 70/97] Loss=0.34269.\n",
            "[Epoch 63/200 Batch 80/97] Loss=0.36144.\n",
            "[Epoch 63/200 Batch 90/97] Loss=0.35371.\n",
            "[Epoch 64/200 Batch 10/97] Loss=0.35549.\n",
            "[Epoch 64/200 Batch 20/97] Loss=0.36838.\n",
            "[Epoch 64/200 Batch 30/97] Loss=0.32542.\n",
            "[Epoch 64/200 Batch 40/97] Loss=0.34026.\n",
            "[Epoch 64/200 Batch 50/97] Loss=0.34263.\n",
            "[Epoch 64/200 Batch 60/97] Loss=0.33709.\n",
            "[Epoch 64/200 Batch 70/97] Loss=0.33378.\n",
            "[Epoch 64/200 Batch 80/97] Loss=0.33095.\n",
            "[Epoch 64/200 Batch 90/97] Loss=0.33012.\n",
            "[Epoch 65/200 Batch 10/97] Loss=0.34305.\n",
            "[Epoch 65/200 Batch 20/97] Loss=0.33310.\n",
            "[Epoch 65/200 Batch 30/97] Loss=0.31480.\n",
            "[Epoch 65/200 Batch 40/97] Loss=0.33924.\n",
            "[Epoch 65/200 Batch 50/97] Loss=0.33621.\n",
            "[Epoch 65/200 Batch 60/97] Loss=0.33147.\n",
            "[Epoch 65/200 Batch 70/97] Loss=0.34955.\n",
            "[Epoch 65/200 Batch 80/97] Loss=0.35241.\n",
            "[Epoch 65/200 Batch 90/97] Loss=0.33904.\n",
            "[Epoch 66/200 Batch 10/97] Loss=0.33800.\n",
            "[Epoch 66/200 Batch 20/97] Loss=0.32797.\n",
            "[Epoch 66/200 Batch 30/97] Loss=0.34523.\n",
            "[Epoch 66/200 Batch 40/97] Loss=0.31645.\n",
            "[Epoch 66/200 Batch 50/97] Loss=0.36626.\n",
            "[Epoch 66/200 Batch 60/97] Loss=0.33698.\n",
            "[Epoch 66/200 Batch 70/97] Loss=0.32437.\n",
            "[Epoch 66/200 Batch 80/97] Loss=0.34395.\n",
            "[Epoch 66/200 Batch 90/97] Loss=0.35570.\n",
            "[Epoch 67/200 Batch 10/97] Loss=0.32876.\n",
            "[Epoch 67/200 Batch 20/97] Loss=0.34165.\n",
            "[Epoch 67/200 Batch 30/97] Loss=0.34691.\n",
            "[Epoch 67/200 Batch 40/97] Loss=0.32752.\n",
            "[Epoch 67/200 Batch 50/97] Loss=0.32744.\n",
            "[Epoch 67/200 Batch 60/97] Loss=0.34618.\n",
            "[Epoch 67/200 Batch 70/97] Loss=0.33337.\n",
            "[Epoch 67/200 Batch 80/97] Loss=0.35041.\n",
            "[Epoch 67/200 Batch 90/97] Loss=0.34189.\n",
            "[Epoch 68/200 Batch 10/97] Loss=0.34108.\n",
            "[Epoch 68/200 Batch 20/97] Loss=0.33614.\n",
            "[Epoch 68/200 Batch 30/97] Loss=0.35100.\n",
            "[Epoch 68/200 Batch 40/97] Loss=0.32138.\n",
            "[Epoch 68/200 Batch 50/97] Loss=0.33640.\n",
            "[Epoch 68/200 Batch 60/97] Loss=0.33864.\n",
            "[Epoch 68/200 Batch 70/97] Loss=0.33253.\n",
            "[Epoch 68/200 Batch 80/97] Loss=0.30813.\n",
            "[Epoch 68/200 Batch 90/97] Loss=0.31876.\n",
            "[Epoch 69/200 Batch 10/97] Loss=0.33836.\n",
            "[Epoch 69/200 Batch 20/97] Loss=0.36444.\n",
            "[Epoch 69/200 Batch 30/97] Loss=0.31079.\n",
            "[Epoch 69/200 Batch 40/97] Loss=0.33271.\n",
            "[Epoch 69/200 Batch 50/97] Loss=0.35000.\n",
            "[Epoch 69/200 Batch 60/97] Loss=0.34517.\n",
            "[Epoch 69/200 Batch 70/97] Loss=0.36516.\n",
            "[Epoch 69/200 Batch 80/97] Loss=0.36345.\n",
            "[Epoch 69/200 Batch 90/97] Loss=0.34464.\n",
            "[Epoch 70/200 Batch 10/97] Loss=0.33066.\n",
            "[Epoch 70/200 Batch 20/97] Loss=0.33781.\n",
            "[Epoch 70/200 Batch 30/97] Loss=0.32651.\n",
            "[Epoch 70/200 Batch 40/97] Loss=0.33232.\n",
            "[Epoch 70/200 Batch 50/97] Loss=0.32108.\n",
            "[Epoch 70/200 Batch 60/97] Loss=0.35127.\n",
            "[Epoch 70/200 Batch 70/97] Loss=0.33016.\n",
            "[Epoch 70/200 Batch 80/97] Loss=0.36288.\n",
            "[Epoch 70/200 Batch 90/97] Loss=0.34553.\n",
            "[Epoch 71/200 Batch 10/97] Loss=0.33930.\n",
            "[Epoch 71/200 Batch 20/97] Loss=0.33251.\n",
            "[Epoch 71/200 Batch 30/97] Loss=0.33917.\n",
            "[Epoch 71/200 Batch 40/97] Loss=0.33615.\n",
            "[Epoch 71/200 Batch 50/97] Loss=0.33282.\n",
            "[Epoch 71/200 Batch 60/97] Loss=0.32787.\n",
            "[Epoch 71/200 Batch 70/97] Loss=0.33179.\n",
            "[Epoch 71/200 Batch 80/97] Loss=0.34530.\n",
            "[Epoch 71/200 Batch 90/97] Loss=0.32315.\n",
            "[Epoch 72/200 Batch 10/97] Loss=0.36918.\n",
            "[Epoch 72/200 Batch 20/97] Loss=0.31977.\n",
            "[Epoch 72/200 Batch 30/97] Loss=0.33122.\n",
            "[Epoch 72/200 Batch 40/97] Loss=0.34714.\n",
            "[Epoch 72/200 Batch 50/97] Loss=0.36971.\n",
            "[Epoch 72/200 Batch 60/97] Loss=0.36013.\n",
            "[Epoch 72/200 Batch 70/97] Loss=0.32825.\n",
            "[Epoch 72/200 Batch 80/97] Loss=0.34364.\n",
            "[Epoch 72/200 Batch 90/97] Loss=0.34270.\n",
            "[Epoch 73/200 Batch 10/97] Loss=0.32785.\n",
            "[Epoch 73/200 Batch 20/97] Loss=0.31851.\n",
            "[Epoch 73/200 Batch 30/97] Loss=0.35983.\n",
            "[Epoch 73/200 Batch 40/97] Loss=0.34259.\n",
            "[Epoch 73/200 Batch 50/97] Loss=0.34076.\n",
            "[Epoch 73/200 Batch 60/97] Loss=0.35635.\n",
            "[Epoch 73/200 Batch 70/97] Loss=0.32982.\n",
            "[Epoch 73/200 Batch 80/97] Loss=0.33368.\n",
            "[Epoch 73/200 Batch 90/97] Loss=0.30325.\n",
            "[Epoch 74/200 Batch 10/97] Loss=0.34275.\n",
            "[Epoch 74/200 Batch 20/97] Loss=0.34208.\n",
            "[Epoch 74/200 Batch 30/97] Loss=0.33515.\n",
            "[Epoch 74/200 Batch 40/97] Loss=0.32721.\n",
            "[Epoch 74/200 Batch 50/97] Loss=0.33245.\n",
            "[Epoch 74/200 Batch 60/97] Loss=0.33025.\n",
            "[Epoch 74/200 Batch 70/97] Loss=0.31091.\n",
            "[Epoch 74/200 Batch 80/97] Loss=0.33861.\n",
            "[Epoch 74/200 Batch 90/97] Loss=0.35946.\n",
            "[Epoch 75/200 Batch 10/97] Loss=0.32639.\n",
            "[Epoch 75/200 Batch 20/97] Loss=0.32459.\n",
            "[Epoch 75/200 Batch 30/97] Loss=0.33173.\n",
            "[Epoch 75/200 Batch 40/97] Loss=0.34341.\n",
            "[Epoch 75/200 Batch 50/97] Loss=0.35941.\n",
            "[Epoch 75/200 Batch 60/97] Loss=0.35108.\n",
            "[Epoch 75/200 Batch 70/97] Loss=0.36062.\n",
            "[Epoch 75/200 Batch 80/97] Loss=0.36841.\n",
            "[Epoch 75/200 Batch 90/97] Loss=0.32041.\n",
            "[Epoch 76/200 Batch 10/97] Loss=0.33358.\n",
            "[Epoch 76/200 Batch 20/97] Loss=0.35724.\n",
            "[Epoch 76/200 Batch 30/97] Loss=0.34561.\n",
            "[Epoch 76/200 Batch 40/97] Loss=0.34943.\n",
            "[Epoch 76/200 Batch 50/97] Loss=0.34518.\n",
            "[Epoch 76/200 Batch 60/97] Loss=0.35260.\n",
            "[Epoch 76/200 Batch 70/97] Loss=0.32812.\n",
            "[Epoch 76/200 Batch 80/97] Loss=0.32338.\n",
            "[Epoch 76/200 Batch 90/97] Loss=0.34876.\n",
            "[Epoch 77/200 Batch 10/97] Loss=0.34913.\n",
            "[Epoch 77/200 Batch 20/97] Loss=0.33580.\n",
            "[Epoch 77/200 Batch 30/97] Loss=0.32798.\n",
            "[Epoch 77/200 Batch 40/97] Loss=0.34074.\n",
            "[Epoch 77/200 Batch 50/97] Loss=0.34519.\n",
            "[Epoch 77/200 Batch 60/97] Loss=0.35195.\n",
            "[Epoch 77/200 Batch 70/97] Loss=0.33845.\n",
            "[Epoch 77/200 Batch 80/97] Loss=0.32441.\n",
            "[Epoch 77/200 Batch 90/97] Loss=0.33965.\n",
            "[Epoch 78/200 Batch 10/97] Loss=0.32567.\n",
            "[Epoch 78/200 Batch 20/97] Loss=0.32867.\n",
            "[Epoch 78/200 Batch 30/97] Loss=0.34118.\n",
            "[Epoch 78/200 Batch 40/97] Loss=0.33107.\n",
            "[Epoch 78/200 Batch 50/97] Loss=0.33377.\n",
            "[Epoch 78/200 Batch 60/97] Loss=0.33944.\n",
            "[Epoch 78/200 Batch 70/97] Loss=0.34305.\n",
            "[Epoch 78/200 Batch 80/97] Loss=0.32710.\n",
            "[Epoch 78/200 Batch 90/97] Loss=0.35779.\n",
            "[Epoch 79/200 Batch 10/97] Loss=0.33761.\n",
            "[Epoch 79/200 Batch 20/97] Loss=0.37046.\n",
            "[Epoch 79/200 Batch 30/97] Loss=0.35024.\n",
            "[Epoch 79/200 Batch 40/97] Loss=0.32389.\n",
            "[Epoch 79/200 Batch 50/97] Loss=0.35606.\n",
            "[Epoch 79/200 Batch 60/97] Loss=0.33196.\n",
            "[Epoch 79/200 Batch 70/97] Loss=0.33070.\n",
            "[Epoch 79/200 Batch 80/97] Loss=0.32689.\n",
            "[Epoch 79/200 Batch 90/97] Loss=0.33185.\n",
            "[Epoch 80/200 Batch 10/97] Loss=0.34535.\n",
            "[Epoch 80/200 Batch 20/97] Loss=0.33937.\n",
            "[Epoch 80/200 Batch 30/97] Loss=0.33273.\n",
            "[Epoch 80/200 Batch 40/97] Loss=0.34352.\n",
            "[Epoch 80/200 Batch 50/97] Loss=0.32642.\n",
            "[Epoch 80/200 Batch 60/97] Loss=0.34500.\n",
            "[Epoch 80/200 Batch 70/97] Loss=0.32993.\n",
            "[Epoch 80/200 Batch 80/97] Loss=0.33766.\n",
            "[Epoch 80/200 Batch 90/97] Loss=0.33987.\n",
            "[Epoch 81/200 Batch 10/97] Loss=0.31803.\n",
            "[Epoch 81/200 Batch 20/97] Loss=0.34262.\n",
            "[Epoch 81/200 Batch 30/97] Loss=0.33956.\n",
            "[Epoch 81/200 Batch 40/97] Loss=0.34099.\n",
            "[Epoch 81/200 Batch 50/97] Loss=0.31752.\n",
            "[Epoch 81/200 Batch 60/97] Loss=0.32952.\n",
            "[Epoch 81/200 Batch 70/97] Loss=0.34426.\n",
            "[Epoch 81/200 Batch 80/97] Loss=0.34288.\n",
            "[Epoch 81/200 Batch 90/97] Loss=0.33089.\n",
            "[Epoch 82/200 Batch 10/97] Loss=0.33056.\n",
            "[Epoch 82/200 Batch 20/97] Loss=0.32592.\n",
            "[Epoch 82/200 Batch 30/97] Loss=0.33403.\n",
            "[Epoch 82/200 Batch 40/97] Loss=0.33301.\n",
            "[Epoch 82/200 Batch 50/97] Loss=0.32672.\n",
            "[Epoch 82/200 Batch 60/97] Loss=0.33499.\n",
            "[Epoch 82/200 Batch 70/97] Loss=0.31058.\n",
            "[Epoch 82/200 Batch 80/97] Loss=0.33795.\n",
            "[Epoch 82/200 Batch 90/97] Loss=0.32281.\n",
            "[Epoch 83/200 Batch 10/97] Loss=0.33368.\n",
            "[Epoch 83/200 Batch 20/97] Loss=0.33201.\n",
            "[Epoch 83/200 Batch 30/97] Loss=0.32432.\n",
            "[Epoch 83/200 Batch 40/97] Loss=0.34859.\n",
            "[Epoch 83/200 Batch 50/97] Loss=0.33555.\n",
            "[Epoch 83/200 Batch 60/97] Loss=0.32012.\n",
            "[Epoch 83/200 Batch 70/97] Loss=0.33342.\n",
            "[Epoch 83/200 Batch 80/97] Loss=0.34071.\n",
            "[Epoch 83/200 Batch 90/97] Loss=0.33930.\n",
            "[Epoch 84/200 Batch 10/97] Loss=0.31524.\n",
            "[Epoch 84/200 Batch 20/97] Loss=0.34360.\n",
            "[Epoch 84/200 Batch 30/97] Loss=0.34416.\n",
            "[Epoch 84/200 Batch 40/97] Loss=0.33669.\n",
            "[Epoch 84/200 Batch 50/97] Loss=0.35733.\n",
            "[Epoch 84/200 Batch 60/97] Loss=0.33423.\n",
            "[Epoch 84/200 Batch 70/97] Loss=0.33008.\n",
            "[Epoch 84/200 Batch 80/97] Loss=0.34926.\n",
            "[Epoch 84/200 Batch 90/97] Loss=0.31587.\n",
            "[Epoch 85/200 Batch 10/97] Loss=0.33877.\n",
            "[Epoch 85/200 Batch 20/97] Loss=0.35898.\n",
            "[Epoch 85/200 Batch 30/97] Loss=0.32243.\n",
            "[Epoch 85/200 Batch 40/97] Loss=0.34632.\n",
            "[Epoch 85/200 Batch 50/97] Loss=0.34203.\n",
            "[Epoch 85/200 Batch 60/97] Loss=0.32948.\n",
            "[Epoch 85/200 Batch 70/97] Loss=0.33513.\n",
            "[Epoch 85/200 Batch 80/97] Loss=0.33922.\n",
            "[Epoch 85/200 Batch 90/97] Loss=0.33518.\n",
            "[Epoch 86/200 Batch 10/97] Loss=0.32486.\n",
            "[Epoch 86/200 Batch 20/97] Loss=0.32778.\n",
            "[Epoch 86/200 Batch 30/97] Loss=0.32735.\n",
            "[Epoch 86/200 Batch 40/97] Loss=0.35049.\n",
            "[Epoch 86/200 Batch 50/97] Loss=0.33813.\n",
            "[Epoch 86/200 Batch 60/97] Loss=0.32875.\n",
            "[Epoch 86/200 Batch 70/97] Loss=0.32328.\n",
            "[Epoch 86/200 Batch 80/97] Loss=0.30514.\n",
            "[Epoch 86/200 Batch 90/97] Loss=0.32668.\n",
            "[Epoch 87/200 Batch 10/97] Loss=0.31696.\n",
            "[Epoch 87/200 Batch 20/97] Loss=0.32412.\n",
            "[Epoch 87/200 Batch 30/97] Loss=0.34348.\n",
            "[Epoch 87/200 Batch 40/97] Loss=0.33083.\n",
            "[Epoch 87/200 Batch 50/97] Loss=0.32638.\n",
            "[Epoch 87/200 Batch 60/97] Loss=0.33495.\n",
            "[Epoch 87/200 Batch 70/97] Loss=0.30421.\n",
            "[Epoch 87/200 Batch 80/97] Loss=0.34399.\n",
            "[Epoch 87/200 Batch 90/97] Loss=0.33491.\n",
            "[Epoch 88/200 Batch 10/97] Loss=0.32231.\n",
            "[Epoch 88/200 Batch 20/97] Loss=0.31985.\n",
            "[Epoch 88/200 Batch 30/97] Loss=0.31471.\n",
            "[Epoch 88/200 Batch 40/97] Loss=0.34172.\n",
            "[Epoch 88/200 Batch 50/97] Loss=0.34100.\n",
            "[Epoch 88/200 Batch 60/97] Loss=0.31200.\n",
            "[Epoch 88/200 Batch 70/97] Loss=0.32868.\n",
            "[Epoch 88/200 Batch 80/97] Loss=0.32595.\n",
            "[Epoch 88/200 Batch 90/97] Loss=0.31747.\n",
            "[Epoch 89/200 Batch 10/97] Loss=0.31221.\n",
            "[Epoch 89/200 Batch 20/97] Loss=0.33755.\n",
            "[Epoch 89/200 Batch 30/97] Loss=0.32813.\n",
            "[Epoch 89/200 Batch 40/97] Loss=0.32798.\n",
            "[Epoch 89/200 Batch 50/97] Loss=0.32592.\n",
            "[Epoch 89/200 Batch 60/97] Loss=0.32105.\n",
            "[Epoch 89/200 Batch 70/97] Loss=0.31699.\n",
            "[Epoch 89/200 Batch 80/97] Loss=0.31552.\n",
            "[Epoch 89/200 Batch 90/97] Loss=0.33138.\n",
            "[Epoch 90/200 Batch 10/97] Loss=0.32746.\n",
            "[Epoch 90/200 Batch 20/97] Loss=0.34860.\n",
            "[Epoch 90/200 Batch 30/97] Loss=0.34829.\n",
            "[Epoch 90/200 Batch 40/97] Loss=0.33698.\n",
            "[Epoch 90/200 Batch 50/97] Loss=0.30425.\n",
            "[Epoch 90/200 Batch 60/97] Loss=0.32857.\n",
            "[Epoch 90/200 Batch 70/97] Loss=0.32602.\n",
            "[Epoch 90/200 Batch 80/97] Loss=0.33706.\n",
            "[Epoch 90/200 Batch 90/97] Loss=0.33997.\n",
            "[Epoch 91/200 Batch 10/97] Loss=0.30590.\n",
            "[Epoch 91/200 Batch 20/97] Loss=0.31982.\n",
            "[Epoch 91/200 Batch 30/97] Loss=0.33627.\n",
            "[Epoch 91/200 Batch 40/97] Loss=0.31384.\n",
            "[Epoch 91/200 Batch 50/97] Loss=0.33813.\n",
            "[Epoch 91/200 Batch 60/97] Loss=0.35213.\n",
            "[Epoch 91/200 Batch 70/97] Loss=0.31944.\n",
            "[Epoch 91/200 Batch 80/97] Loss=0.31466.\n",
            "[Epoch 91/200 Batch 90/97] Loss=0.31070.\n",
            "[Epoch 92/200 Batch 10/97] Loss=0.31575.\n",
            "[Epoch 92/200 Batch 20/97] Loss=0.35438.\n",
            "[Epoch 92/200 Batch 30/97] Loss=0.34588.\n",
            "[Epoch 92/200 Batch 40/97] Loss=0.32257.\n",
            "[Epoch 92/200 Batch 50/97] Loss=0.31349.\n",
            "[Epoch 92/200 Batch 60/97] Loss=0.33361.\n",
            "[Epoch 92/200 Batch 70/97] Loss=0.32160.\n",
            "[Epoch 92/200 Batch 80/97] Loss=0.31792.\n",
            "[Epoch 92/200 Batch 90/97] Loss=0.33459.\n",
            "[Epoch 93/200 Batch 10/97] Loss=0.33841.\n",
            "[Epoch 93/200 Batch 20/97] Loss=0.34032.\n",
            "[Epoch 93/200 Batch 30/97] Loss=0.32873.\n",
            "[Epoch 93/200 Batch 40/97] Loss=0.33646.\n",
            "[Epoch 93/200 Batch 50/97] Loss=0.31925.\n",
            "[Epoch 93/200 Batch 60/97] Loss=0.33010.\n",
            "[Epoch 93/200 Batch 70/97] Loss=0.34206.\n",
            "[Epoch 93/200 Batch 80/97] Loss=0.34491.\n",
            "[Epoch 93/200 Batch 90/97] Loss=0.33505.\n",
            "[Epoch 94/200 Batch 10/97] Loss=0.30570.\n",
            "[Epoch 94/200 Batch 20/97] Loss=0.31763.\n",
            "[Epoch 94/200 Batch 30/97] Loss=0.31900.\n",
            "[Epoch 94/200 Batch 40/97] Loss=0.31602.\n",
            "[Epoch 94/200 Batch 50/97] Loss=0.33540.\n",
            "[Epoch 94/200 Batch 60/97] Loss=0.32072.\n",
            "[Epoch 94/200 Batch 70/97] Loss=0.32807.\n",
            "[Epoch 94/200 Batch 80/97] Loss=0.31368.\n",
            "[Epoch 94/200 Batch 90/97] Loss=0.31442.\n",
            "[Epoch 95/200 Batch 10/97] Loss=0.31326.\n",
            "[Epoch 95/200 Batch 20/97] Loss=0.32769.\n",
            "[Epoch 95/200 Batch 30/97] Loss=0.33142.\n",
            "[Epoch 95/200 Batch 40/97] Loss=0.35124.\n",
            "[Epoch 95/200 Batch 50/97] Loss=0.32590.\n",
            "[Epoch 95/200 Batch 60/97] Loss=0.32938.\n",
            "[Epoch 95/200 Batch 70/97] Loss=0.33292.\n",
            "[Epoch 95/200 Batch 80/97] Loss=0.34943.\n",
            "[Epoch 95/200 Batch 90/97] Loss=0.33048.\n",
            "[Epoch 96/200 Batch 10/97] Loss=0.32919.\n",
            "[Epoch 96/200 Batch 20/97] Loss=0.34469.\n",
            "[Epoch 96/200 Batch 30/97] Loss=0.31642.\n",
            "[Epoch 96/200 Batch 40/97] Loss=0.33367.\n",
            "[Epoch 96/200 Batch 50/97] Loss=0.33090.\n",
            "[Epoch 96/200 Batch 60/97] Loss=0.36224.\n",
            "[Epoch 96/200 Batch 70/97] Loss=0.34039.\n",
            "[Epoch 96/200 Batch 80/97] Loss=0.32251.\n",
            "[Epoch 96/200 Batch 90/97] Loss=0.33203.\n",
            "[Epoch 97/200 Batch 10/97] Loss=0.32456.\n",
            "[Epoch 97/200 Batch 20/97] Loss=0.31831.\n",
            "[Epoch 97/200 Batch 30/97] Loss=0.32390.\n",
            "[Epoch 97/200 Batch 40/97] Loss=0.33773.\n",
            "[Epoch 97/200 Batch 50/97] Loss=0.29656.\n",
            "[Epoch 97/200 Batch 60/97] Loss=0.34223.\n",
            "[Epoch 97/200 Batch 70/97] Loss=0.34459.\n",
            "[Epoch 97/200 Batch 80/97] Loss=0.31721.\n",
            "[Epoch 97/200 Batch 90/97] Loss=0.33115.\n",
            "[Epoch 98/200 Batch 10/97] Loss=0.35745.\n",
            "[Epoch 98/200 Batch 20/97] Loss=0.33876.\n",
            "[Epoch 98/200 Batch 30/97] Loss=0.34895.\n",
            "[Epoch 98/200 Batch 40/97] Loss=0.32155.\n",
            "[Epoch 98/200 Batch 50/97] Loss=0.34349.\n",
            "[Epoch 98/200 Batch 60/97] Loss=0.32669.\n",
            "[Epoch 98/200 Batch 70/97] Loss=0.32683.\n",
            "[Epoch 98/200 Batch 80/97] Loss=0.32719.\n",
            "[Epoch 98/200 Batch 90/97] Loss=0.34057.\n",
            "[Epoch 99/200 Batch 10/97] Loss=0.33957.\n",
            "[Epoch 99/200 Batch 20/97] Loss=0.31027.\n",
            "[Epoch 99/200 Batch 30/97] Loss=0.32917.\n",
            "[Epoch 99/200 Batch 40/97] Loss=0.31977.\n",
            "[Epoch 99/200 Batch 50/97] Loss=0.33786.\n",
            "[Epoch 99/200 Batch 60/97] Loss=0.30933.\n",
            "[Epoch 99/200 Batch 70/97] Loss=0.32370.\n",
            "[Epoch 99/200 Batch 80/97] Loss=0.33336.\n",
            "[Epoch 99/200 Batch 90/97] Loss=0.31565.\n",
            "[Epoch 100/200 Batch 10/97] Loss=0.33699.\n",
            "[Epoch 100/200 Batch 20/97] Loss=0.33640.\n",
            "[Epoch 100/200 Batch 30/97] Loss=0.32114.\n",
            "[Epoch 100/200 Batch 40/97] Loss=0.30710.\n",
            "[Epoch 100/200 Batch 50/97] Loss=0.33257.\n",
            "[Epoch 100/200 Batch 60/97] Loss=0.32983.\n",
            "[Epoch 100/200 Batch 70/97] Loss=0.33679.\n",
            "[Epoch 100/200 Batch 80/97] Loss=0.31108.\n",
            "[Epoch 100/200 Batch 90/97] Loss=0.34131.\n",
            "Weights of f saved.\n",
            "[Epoch 101/200 Batch 10/97] Loss=0.32917.\n",
            "[Epoch 101/200 Batch 20/97] Loss=0.33248.\n",
            "[Epoch 101/200 Batch 30/97] Loss=0.33797.\n",
            "[Epoch 101/200 Batch 40/97] Loss=0.31994.\n",
            "[Epoch 101/200 Batch 50/97] Loss=0.33399.\n",
            "[Epoch 101/200 Batch 60/97] Loss=0.32918.\n",
            "[Epoch 101/200 Batch 70/97] Loss=0.33730.\n",
            "[Epoch 101/200 Batch 80/97] Loss=0.34996.\n",
            "[Epoch 101/200 Batch 90/97] Loss=0.34298.\n",
            "[Epoch 102/200 Batch 10/97] Loss=0.33058.\n",
            "[Epoch 102/200 Batch 20/97] Loss=0.31764.\n",
            "[Epoch 102/200 Batch 30/97] Loss=0.31911.\n",
            "[Epoch 102/200 Batch 40/97] Loss=0.33646.\n",
            "[Epoch 102/200 Batch 50/97] Loss=0.31345.\n",
            "[Epoch 102/200 Batch 60/97] Loss=0.35010.\n",
            "[Epoch 102/200 Batch 70/97] Loss=0.33164.\n",
            "[Epoch 102/200 Batch 80/97] Loss=0.32972.\n",
            "[Epoch 102/200 Batch 90/97] Loss=0.34237.\n",
            "[Epoch 103/200 Batch 10/97] Loss=0.32500.\n",
            "[Epoch 103/200 Batch 20/97] Loss=0.33788.\n",
            "[Epoch 103/200 Batch 30/97] Loss=0.33716.\n",
            "[Epoch 103/200 Batch 40/97] Loss=0.32633.\n",
            "[Epoch 103/200 Batch 50/97] Loss=0.31812.\n",
            "[Epoch 103/200 Batch 60/97] Loss=0.31687.\n",
            "[Epoch 103/200 Batch 70/97] Loss=0.32343.\n",
            "[Epoch 103/200 Batch 80/97] Loss=0.32240.\n",
            "[Epoch 103/200 Batch 90/97] Loss=0.31333.\n",
            "[Epoch 104/200 Batch 10/97] Loss=0.33135.\n",
            "[Epoch 104/200 Batch 20/97] Loss=0.30945.\n",
            "[Epoch 104/200 Batch 30/97] Loss=0.32671.\n",
            "[Epoch 104/200 Batch 40/97] Loss=0.34075.\n",
            "[Epoch 104/200 Batch 50/97] Loss=0.33966.\n",
            "[Epoch 104/200 Batch 60/97] Loss=0.32386.\n",
            "[Epoch 104/200 Batch 70/97] Loss=0.30724.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpDu5RYB8Dd2"
      },
      "source": [
        "### linearevaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0_QX6oGGir6"
      },
      "source": [
        "import os\r\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\n",
        "\r\n",
        "import argparse\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "#from datasets import CIFAR10\r\n",
        "#from models import ResNet18, ResNet34, ClassificationHead\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "encoders = {'resnet18': ResNet18, 'resnet34': ResNet34}\r\n",
        "\r\n",
        "\r\n",
        "def compute_test_accuracy(data, f_net, c_net):\r\n",
        "    batch_size = 500\r\n",
        "    num_batches = data.num_test_images // batch_size\r\n",
        "\r\n",
        "    num_correct_predictions = 0\r\n",
        "    for batch_id in range(num_batches):\r\n",
        "        x, y = data.get_batch_testing(batch_id, batch_size)\r\n",
        "        h = f_net(x, training=False)\r\n",
        "        y_pred_logits = c_net(h)\r\n",
        "        y_pred_labels = tf.argmax(y_pred_logits, axis=1, output_type=tf.int32)\r\n",
        "\r\n",
        "        num_correct_predictions += tf.reduce_sum(tf.cast(tf.equal(y_pred_labels, y), tf.int32))\r\n",
        "\r\n",
        "    return tf.cast(num_correct_predictions / data.num_test_images, tf.float32)\r\n",
        "\r\n",
        "\r\n",
        "def main(encoder,encoder_weights):\r\n",
        "\r\n",
        "    # Load CIFAR-10 dataset\r\n",
        "    data = CIFAR10()\r\n",
        "\r\n",
        "    # Define hyperparameters\r\n",
        "    num_epochs = 50\r\n",
        "    batch_size = 512\r\n",
        "\r\n",
        "    # Instantiate networks f and c\r\n",
        "    f_net = encoders[encoder]()\r\n",
        "    c_net = ClassificationHead()\r\n",
        "\r\n",
        "    # Initialize the weights of f and c\r\n",
        "    x, y = data.get_batch_finetuning(batch_id=0, batch_size=batch_size)\r\n",
        "    h = f_net(x, training=False)\r\n",
        "    print('Shape of h:', h.shape)\r\n",
        "    s = c_net(h)\r\n",
        "    print('Shape of s:', s.shape)\r\n",
        "\r\n",
        "    # Load the weights of f from pretraining\r\n",
        "    f_net.load_weights(encoder_weights)\r\n",
        "    print('Weights of f loaded.')\r\n",
        "\r\n",
        "\r\n",
        "    # Define optimizer\r\n",
        "    batches_per_epoch = data.num_train_images // batch_size\r\n",
        "    total_update_steps = num_epochs * batches_per_epoch\r\n",
        "    lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(5e-2, total_update_steps, 5e-4, power=2)\r\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\r\n",
        "\r\n",
        "    \r\n",
        "    @tf.function\r\n",
        "    def train_step_evaluation(x, y):  # (bs, 32, 32, 3), (bs)\r\n",
        "\r\n",
        "        # Forward pass\r\n",
        "        with tf.GradientTape() as tape:\r\n",
        "            h = f_net(x, training=False)  # (bs, 512)\r\n",
        "            y_pred_logits = c_net(h)  # (bs, 10)\r\n",
        "            loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=y_pred_logits))\r\n",
        "        \r\n",
        "        # Backward pass\r\n",
        "        grads = tape.gradient(loss, c_net.trainable_variables)\r\n",
        "        opt.apply_gradients(zip(grads, c_net.trainable_variables))\r\n",
        "\r\n",
        "        return loss\r\n",
        "\r\n",
        "\r\n",
        "    log_every = 10  # batches\r\n",
        "    for epoch_id in range(num_epochs):\r\n",
        "        data.shuffle_training_data()\r\n",
        "        \r\n",
        "        for batch_id in range(batches_per_epoch):\r\n",
        "            x, y = data.get_batch_finetuning(batch_id, batch_size)\r\n",
        "            loss = train_step_evaluation(x, y)\r\n",
        "            if (batch_id + 1) % log_every == 0:\r\n",
        "                print('[Epoch {}/{} Batch {}/{}] Loss: {:.4f}'.format(epoch_id+1, num_epochs, batch_id+1, batches_per_epoch, loss))\r\n",
        "    \r\n",
        "    # Compute classification accuracy on test set\r\n",
        "    test_accuracy = compute_test_accuracy(data, f_net, c_net)\r\n",
        "    print('Test Accuracy: {:.4f}'.format(test_accuracy))\r\n",
        "    \r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsXe1uv98R6g",
        "outputId": "fe376078-1a26-48e7-e0cc-73308e65f4c4"
      },
      "source": [
        "\r\n",
        "# parser = argparse.ArgumentParser()\r\n",
        "\r\n",
        "# parser.add_argument('--encoder', type=str, required=True, choices=['resnet18', 'resnet34'], help='Encoder architecture')\r\n",
        "# parser.add_argument('--encoder_weights', type=str, help='Encoder weights')\r\n",
        "\r\n",
        "# args = parser.parse_args()\r\n",
        "main('resnet18','/content/drive/MyDrive/dataset/f_online_100.h5')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n",
            "Shape of h: (512, 512)\n",
            "Shape of s: (512, 10)\n",
            "Weights of f loaded.\n",
            "[Epoch 1/50 Batch 10/97] Loss: 3.3132\n",
            "[Epoch 1/50 Batch 20/97] Loss: 2.0262\n",
            "[Epoch 1/50 Batch 30/97] Loss: 1.4943\n",
            "[Epoch 1/50 Batch 40/97] Loss: 1.4179\n",
            "[Epoch 1/50 Batch 50/97] Loss: 1.0761\n",
            "[Epoch 1/50 Batch 60/97] Loss: 1.0093\n",
            "[Epoch 1/50 Batch 70/97] Loss: 1.0558\n",
            "[Epoch 1/50 Batch 80/97] Loss: 0.7877\n",
            "[Epoch 1/50 Batch 90/97] Loss: 0.8801\n",
            "[Epoch 2/50 Batch 10/97] Loss: 0.7555\n",
            "[Epoch 2/50 Batch 20/97] Loss: 0.8941\n",
            "[Epoch 2/50 Batch 30/97] Loss: 0.8735\n",
            "[Epoch 2/50 Batch 40/97] Loss: 0.7674\n",
            "[Epoch 2/50 Batch 50/97] Loss: 0.7736\n",
            "[Epoch 2/50 Batch 60/97] Loss: 0.8255\n",
            "[Epoch 2/50 Batch 70/97] Loss: 0.8255\n",
            "[Epoch 2/50 Batch 80/97] Loss: 0.8494\n",
            "[Epoch 2/50 Batch 90/97] Loss: 0.7836\n",
            "[Epoch 3/50 Batch 10/97] Loss: 0.8952\n",
            "[Epoch 3/50 Batch 20/97] Loss: 0.8055\n",
            "[Epoch 3/50 Batch 30/97] Loss: 0.7151\n",
            "[Epoch 3/50 Batch 40/97] Loss: 0.7777\n",
            "[Epoch 3/50 Batch 50/97] Loss: 0.8781\n",
            "[Epoch 3/50 Batch 60/97] Loss: 0.8288\n",
            "[Epoch 3/50 Batch 70/97] Loss: 0.8704\n",
            "[Epoch 3/50 Batch 80/97] Loss: 0.6703\n",
            "[Epoch 3/50 Batch 90/97] Loss: 0.8395\n",
            "[Epoch 4/50 Batch 10/97] Loss: 0.8081\n",
            "[Epoch 4/50 Batch 20/97] Loss: 0.7888\n",
            "[Epoch 4/50 Batch 30/97] Loss: 0.8209\n",
            "[Epoch 4/50 Batch 40/97] Loss: 0.9338\n",
            "[Epoch 4/50 Batch 50/97] Loss: 0.9946\n",
            "[Epoch 4/50 Batch 60/97] Loss: 0.9444\n",
            "[Epoch 4/50 Batch 70/97] Loss: 0.8242\n",
            "[Epoch 4/50 Batch 80/97] Loss: 0.7061\n",
            "[Epoch 4/50 Batch 90/97] Loss: 0.7574\n",
            "[Epoch 5/50 Batch 10/97] Loss: 0.8865\n",
            "[Epoch 5/50 Batch 20/97] Loss: 0.8703\n",
            "[Epoch 5/50 Batch 30/97] Loss: 0.8162\n",
            "[Epoch 5/50 Batch 40/97] Loss: 0.8552\n",
            "[Epoch 5/50 Batch 50/97] Loss: 0.8794\n",
            "[Epoch 5/50 Batch 60/97] Loss: 0.8901\n",
            "[Epoch 5/50 Batch 70/97] Loss: 0.8266\n",
            "[Epoch 5/50 Batch 80/97] Loss: 0.8571\n",
            "[Epoch 5/50 Batch 90/97] Loss: 0.8235\n",
            "[Epoch 6/50 Batch 10/97] Loss: 0.7258\n",
            "[Epoch 6/50 Batch 20/97] Loss: 0.7358\n",
            "[Epoch 6/50 Batch 30/97] Loss: 0.9351\n",
            "[Epoch 6/50 Batch 40/97] Loss: 0.7876\n",
            "[Epoch 6/50 Batch 50/97] Loss: 0.7221\n",
            "[Epoch 6/50 Batch 60/97] Loss: 0.8519\n",
            "[Epoch 6/50 Batch 70/97] Loss: 0.8599\n",
            "[Epoch 6/50 Batch 80/97] Loss: 0.9074\n",
            "[Epoch 6/50 Batch 90/97] Loss: 0.8204\n",
            "[Epoch 7/50 Batch 10/97] Loss: 0.7291\n",
            "[Epoch 7/50 Batch 20/97] Loss: 0.8389\n",
            "[Epoch 7/50 Batch 30/97] Loss: 0.8170\n",
            "[Epoch 7/50 Batch 40/97] Loss: 0.8682\n",
            "[Epoch 7/50 Batch 50/97] Loss: 0.8182\n",
            "[Epoch 7/50 Batch 60/97] Loss: 0.8667\n",
            "[Epoch 7/50 Batch 70/97] Loss: 0.7483\n",
            "[Epoch 7/50 Batch 80/97] Loss: 0.7757\n",
            "[Epoch 7/50 Batch 90/97] Loss: 0.8317\n",
            "[Epoch 8/50 Batch 10/97] Loss: 0.7975\n",
            "[Epoch 8/50 Batch 20/97] Loss: 0.8281\n",
            "[Epoch 8/50 Batch 30/97] Loss: 0.7221\n",
            "[Epoch 8/50 Batch 40/97] Loss: 0.7485\n",
            "[Epoch 8/50 Batch 50/97] Loss: 0.7772\n",
            "[Epoch 8/50 Batch 60/97] Loss: 0.7990\n",
            "[Epoch 8/50 Batch 70/97] Loss: 0.7589\n",
            "[Epoch 8/50 Batch 80/97] Loss: 0.7256\n",
            "[Epoch 8/50 Batch 90/97] Loss: 0.8632\n",
            "[Epoch 9/50 Batch 10/97] Loss: 0.7897\n",
            "[Epoch 9/50 Batch 20/97] Loss: 0.7433\n",
            "[Epoch 9/50 Batch 30/97] Loss: 0.7353\n",
            "[Epoch 9/50 Batch 40/97] Loss: 0.7987\n",
            "[Epoch 9/50 Batch 50/97] Loss: 0.8227\n",
            "[Epoch 9/50 Batch 60/97] Loss: 0.7901\n",
            "[Epoch 9/50 Batch 70/97] Loss: 0.7328\n",
            "[Epoch 9/50 Batch 80/97] Loss: 0.7198\n",
            "[Epoch 9/50 Batch 90/97] Loss: 0.7357\n",
            "[Epoch 10/50 Batch 10/97] Loss: 0.7959\n",
            "[Epoch 10/50 Batch 20/97] Loss: 0.6879\n",
            "[Epoch 10/50 Batch 30/97] Loss: 0.6758\n",
            "[Epoch 10/50 Batch 40/97] Loss: 0.7426\n",
            "[Epoch 10/50 Batch 50/97] Loss: 0.7615\n",
            "[Epoch 10/50 Batch 60/97] Loss: 0.7283\n",
            "[Epoch 10/50 Batch 70/97] Loss: 0.8393\n",
            "[Epoch 10/50 Batch 80/97] Loss: 0.7399\n",
            "[Epoch 10/50 Batch 90/97] Loss: 0.7624\n",
            "[Epoch 11/50 Batch 10/97] Loss: 0.7643\n",
            "[Epoch 11/50 Batch 20/97] Loss: 0.7397\n",
            "[Epoch 11/50 Batch 30/97] Loss: 0.7201\n",
            "[Epoch 11/50 Batch 40/97] Loss: 0.7895\n",
            "[Epoch 11/50 Batch 50/97] Loss: 0.6893\n",
            "[Epoch 11/50 Batch 60/97] Loss: 0.6851\n",
            "[Epoch 11/50 Batch 70/97] Loss: 0.7169\n",
            "[Epoch 11/50 Batch 80/97] Loss: 0.7704\n",
            "[Epoch 11/50 Batch 90/97] Loss: 0.8882\n",
            "[Epoch 12/50 Batch 10/97] Loss: 0.7044\n",
            "[Epoch 12/50 Batch 20/97] Loss: 0.8361\n",
            "[Epoch 12/50 Batch 30/97] Loss: 0.9160\n",
            "[Epoch 12/50 Batch 40/97] Loss: 0.7256\n",
            "[Epoch 12/50 Batch 50/97] Loss: 0.8063\n",
            "[Epoch 12/50 Batch 60/97] Loss: 0.7430\n",
            "[Epoch 12/50 Batch 70/97] Loss: 0.7236\n",
            "[Epoch 12/50 Batch 80/97] Loss: 0.7971\n",
            "[Epoch 12/50 Batch 90/97] Loss: 0.7385\n",
            "[Epoch 13/50 Batch 10/97] Loss: 0.7142\n",
            "[Epoch 13/50 Batch 20/97] Loss: 0.8682\n",
            "[Epoch 13/50 Batch 30/97] Loss: 0.7928\n",
            "[Epoch 13/50 Batch 40/97] Loss: 0.7316\n",
            "[Epoch 13/50 Batch 50/97] Loss: 0.6526\n",
            "[Epoch 13/50 Batch 60/97] Loss: 0.7985\n",
            "[Epoch 13/50 Batch 70/97] Loss: 0.8218\n",
            "[Epoch 13/50 Batch 80/97] Loss: 0.7831\n",
            "[Epoch 13/50 Batch 90/97] Loss: 0.7630\n",
            "[Epoch 14/50 Batch 10/97] Loss: 0.6976\n",
            "[Epoch 14/50 Batch 20/97] Loss: 0.8616\n",
            "[Epoch 14/50 Batch 30/97] Loss: 0.7421\n",
            "[Epoch 14/50 Batch 40/97] Loss: 0.7999\n",
            "[Epoch 14/50 Batch 50/97] Loss: 0.7102\n",
            "[Epoch 14/50 Batch 60/97] Loss: 0.6544\n",
            "[Epoch 14/50 Batch 70/97] Loss: 0.8457\n",
            "[Epoch 14/50 Batch 80/97] Loss: 0.7591\n",
            "[Epoch 14/50 Batch 90/97] Loss: 0.8348\n",
            "[Epoch 15/50 Batch 10/97] Loss: 0.7154\n",
            "[Epoch 15/50 Batch 20/97] Loss: 0.7605\n",
            "[Epoch 15/50 Batch 30/97] Loss: 0.7951\n",
            "[Epoch 15/50 Batch 40/97] Loss: 0.7797\n",
            "[Epoch 15/50 Batch 50/97] Loss: 0.7127\n",
            "[Epoch 15/50 Batch 60/97] Loss: 0.7282\n",
            "[Epoch 15/50 Batch 70/97] Loss: 0.7464\n",
            "[Epoch 15/50 Batch 80/97] Loss: 0.8144\n",
            "[Epoch 15/50 Batch 90/97] Loss: 0.8365\n",
            "[Epoch 16/50 Batch 10/97] Loss: 0.6799\n",
            "[Epoch 16/50 Batch 20/97] Loss: 0.7335\n",
            "[Epoch 16/50 Batch 30/97] Loss: 0.7284\n",
            "[Epoch 16/50 Batch 40/97] Loss: 0.6415\n",
            "[Epoch 16/50 Batch 50/97] Loss: 0.7704\n",
            "[Epoch 16/50 Batch 60/97] Loss: 0.7807\n",
            "[Epoch 16/50 Batch 70/97] Loss: 0.7879\n",
            "[Epoch 16/50 Batch 80/97] Loss: 0.7114\n",
            "[Epoch 16/50 Batch 90/97] Loss: 0.8193\n",
            "[Epoch 17/50 Batch 10/97] Loss: 0.6664\n",
            "[Epoch 17/50 Batch 20/97] Loss: 0.8269\n",
            "[Epoch 17/50 Batch 30/97] Loss: 0.6629\n",
            "[Epoch 17/50 Batch 40/97] Loss: 0.7255\n",
            "[Epoch 17/50 Batch 50/97] Loss: 0.6416\n",
            "[Epoch 17/50 Batch 60/97] Loss: 0.8311\n",
            "[Epoch 17/50 Batch 70/97] Loss: 0.7223\n",
            "[Epoch 17/50 Batch 80/97] Loss: 0.7226\n",
            "[Epoch 17/50 Batch 90/97] Loss: 0.6979\n",
            "[Epoch 18/50 Batch 10/97] Loss: 0.6931\n",
            "[Epoch 18/50 Batch 20/97] Loss: 0.7324\n",
            "[Epoch 18/50 Batch 30/97] Loss: 0.7627\n",
            "[Epoch 18/50 Batch 40/97] Loss: 0.7098\n",
            "[Epoch 18/50 Batch 50/97] Loss: 0.6663\n",
            "[Epoch 18/50 Batch 60/97] Loss: 0.7107\n",
            "[Epoch 18/50 Batch 70/97] Loss: 0.8168\n",
            "[Epoch 18/50 Batch 80/97] Loss: 0.7576\n",
            "[Epoch 18/50 Batch 90/97] Loss: 0.7167\n",
            "[Epoch 19/50 Batch 10/97] Loss: 0.7207\n",
            "[Epoch 19/50 Batch 20/97] Loss: 0.7101\n",
            "[Epoch 19/50 Batch 30/97] Loss: 0.7793\n",
            "[Epoch 19/50 Batch 40/97] Loss: 0.6866\n",
            "[Epoch 19/50 Batch 50/97] Loss: 0.6774\n",
            "[Epoch 19/50 Batch 60/97] Loss: 0.7204\n",
            "[Epoch 19/50 Batch 70/97] Loss: 0.7381\n",
            "[Epoch 19/50 Batch 80/97] Loss: 0.7291\n",
            "[Epoch 19/50 Batch 90/97] Loss: 0.7285\n",
            "[Epoch 20/50 Batch 10/97] Loss: 0.7952\n",
            "[Epoch 20/50 Batch 20/97] Loss: 0.7140\n",
            "[Epoch 20/50 Batch 30/97] Loss: 0.6860\n",
            "[Epoch 20/50 Batch 40/97] Loss: 0.6754\n",
            "[Epoch 20/50 Batch 50/97] Loss: 0.7343\n",
            "[Epoch 20/50 Batch 60/97] Loss: 0.7614\n",
            "[Epoch 20/50 Batch 70/97] Loss: 0.7253\n",
            "[Epoch 20/50 Batch 80/97] Loss: 0.6840\n",
            "[Epoch 20/50 Batch 90/97] Loss: 0.7959\n",
            "[Epoch 21/50 Batch 10/97] Loss: 0.6979\n",
            "[Epoch 21/50 Batch 20/97] Loss: 0.7656\n",
            "[Epoch 21/50 Batch 30/97] Loss: 0.6455\n",
            "[Epoch 21/50 Batch 40/97] Loss: 0.7518\n",
            "[Epoch 21/50 Batch 50/97] Loss: 0.7272\n",
            "[Epoch 21/50 Batch 60/97] Loss: 0.6839\n",
            "[Epoch 21/50 Batch 70/97] Loss: 0.7379\n",
            "[Epoch 21/50 Batch 80/97] Loss: 0.7490\n",
            "[Epoch 21/50 Batch 90/97] Loss: 0.7079\n",
            "[Epoch 22/50 Batch 10/97] Loss: 0.7228\n",
            "[Epoch 22/50 Batch 20/97] Loss: 0.7594\n",
            "[Epoch 22/50 Batch 30/97] Loss: 0.6511\n",
            "[Epoch 22/50 Batch 40/97] Loss: 0.6905\n",
            "[Epoch 22/50 Batch 50/97] Loss: 0.6847\n",
            "[Epoch 22/50 Batch 60/97] Loss: 0.7520\n",
            "[Epoch 22/50 Batch 70/97] Loss: 0.7420\n",
            "[Epoch 22/50 Batch 80/97] Loss: 0.6535\n",
            "[Epoch 22/50 Batch 90/97] Loss: 0.6938\n",
            "[Epoch 23/50 Batch 10/97] Loss: 0.7127\n",
            "[Epoch 23/50 Batch 20/97] Loss: 0.6593\n",
            "[Epoch 23/50 Batch 30/97] Loss: 0.8291\n",
            "[Epoch 23/50 Batch 40/97] Loss: 0.6775\n",
            "[Epoch 23/50 Batch 50/97] Loss: 0.6810\n",
            "[Epoch 23/50 Batch 60/97] Loss: 0.6374\n",
            "[Epoch 23/50 Batch 70/97] Loss: 0.7293\n",
            "[Epoch 23/50 Batch 80/97] Loss: 0.6735\n",
            "[Epoch 23/50 Batch 90/97] Loss: 0.6411\n",
            "[Epoch 24/50 Batch 10/97] Loss: 0.7512\n",
            "[Epoch 24/50 Batch 20/97] Loss: 0.7153\n",
            "[Epoch 24/50 Batch 30/97] Loss: 0.6458\n",
            "[Epoch 24/50 Batch 40/97] Loss: 0.6735\n",
            "[Epoch 24/50 Batch 50/97] Loss: 0.7973\n",
            "[Epoch 24/50 Batch 60/97] Loss: 0.6752\n",
            "[Epoch 24/50 Batch 70/97] Loss: 0.7503\n",
            "[Epoch 24/50 Batch 80/97] Loss: 0.7066\n",
            "[Epoch 24/50 Batch 90/97] Loss: 0.6692\n",
            "[Epoch 25/50 Batch 10/97] Loss: 0.7901\n",
            "[Epoch 25/50 Batch 20/97] Loss: 0.7364\n",
            "[Epoch 25/50 Batch 30/97] Loss: 0.7815\n",
            "[Epoch 25/50 Batch 40/97] Loss: 0.7483\n",
            "[Epoch 25/50 Batch 50/97] Loss: 0.6455\n",
            "[Epoch 25/50 Batch 60/97] Loss: 0.7111\n",
            "[Epoch 25/50 Batch 70/97] Loss: 0.6665\n",
            "[Epoch 25/50 Batch 80/97] Loss: 0.7436\n",
            "[Epoch 25/50 Batch 90/97] Loss: 0.6260\n",
            "[Epoch 26/50 Batch 10/97] Loss: 0.6858\n",
            "[Epoch 26/50 Batch 20/97] Loss: 0.7727\n",
            "[Epoch 26/50 Batch 30/97] Loss: 0.7095\n",
            "[Epoch 26/50 Batch 40/97] Loss: 0.6900\n",
            "[Epoch 26/50 Batch 50/97] Loss: 0.7353\n",
            "[Epoch 26/50 Batch 60/97] Loss: 0.6376\n",
            "[Epoch 26/50 Batch 70/97] Loss: 0.6847\n",
            "[Epoch 26/50 Batch 80/97] Loss: 0.6783\n",
            "[Epoch 26/50 Batch 90/97] Loss: 0.6866\n",
            "[Epoch 27/50 Batch 10/97] Loss: 0.6440\n",
            "[Epoch 27/50 Batch 20/97] Loss: 0.7117\n",
            "[Epoch 27/50 Batch 30/97] Loss: 0.7065\n",
            "[Epoch 27/50 Batch 40/97] Loss: 0.7400\n",
            "[Epoch 27/50 Batch 50/97] Loss: 0.6722\n",
            "[Epoch 27/50 Batch 60/97] Loss: 0.7495\n",
            "[Epoch 27/50 Batch 70/97] Loss: 0.6350\n",
            "[Epoch 27/50 Batch 80/97] Loss: 0.6448\n",
            "[Epoch 27/50 Batch 90/97] Loss: 0.6847\n",
            "[Epoch 28/50 Batch 10/97] Loss: 0.7087\n",
            "[Epoch 28/50 Batch 20/97] Loss: 0.7086\n",
            "[Epoch 28/50 Batch 30/97] Loss: 0.6718\n",
            "[Epoch 28/50 Batch 40/97] Loss: 0.7026\n",
            "[Epoch 28/50 Batch 50/97] Loss: 0.6413\n",
            "[Epoch 28/50 Batch 60/97] Loss: 0.6381\n",
            "[Epoch 28/50 Batch 70/97] Loss: 0.7697\n",
            "[Epoch 28/50 Batch 80/97] Loss: 0.6256\n",
            "[Epoch 28/50 Batch 90/97] Loss: 0.6802\n",
            "[Epoch 29/50 Batch 10/97] Loss: 0.6352\n",
            "[Epoch 29/50 Batch 20/97] Loss: 0.6484\n",
            "[Epoch 29/50 Batch 30/97] Loss: 0.6486\n",
            "[Epoch 29/50 Batch 40/97] Loss: 0.6703\n",
            "[Epoch 29/50 Batch 50/97] Loss: 0.6572\n",
            "[Epoch 29/50 Batch 60/97] Loss: 0.7178\n",
            "[Epoch 29/50 Batch 70/97] Loss: 0.7933\n",
            "[Epoch 29/50 Batch 80/97] Loss: 0.6819\n",
            "[Epoch 29/50 Batch 90/97] Loss: 0.7106\n",
            "[Epoch 30/50 Batch 10/97] Loss: 0.6984\n",
            "[Epoch 30/50 Batch 20/97] Loss: 0.6454\n",
            "[Epoch 30/50 Batch 30/97] Loss: 0.7223\n",
            "[Epoch 30/50 Batch 40/97] Loss: 0.6968\n",
            "[Epoch 30/50 Batch 50/97] Loss: 0.7014\n",
            "[Epoch 30/50 Batch 60/97] Loss: 0.7432\n",
            "[Epoch 30/50 Batch 70/97] Loss: 0.6339\n",
            "[Epoch 30/50 Batch 80/97] Loss: 0.8084\n",
            "[Epoch 30/50 Batch 90/97] Loss: 0.7196\n",
            "[Epoch 31/50 Batch 10/97] Loss: 0.7112\n",
            "[Epoch 31/50 Batch 20/97] Loss: 0.6474\n",
            "[Epoch 31/50 Batch 30/97] Loss: 0.6723\n",
            "[Epoch 31/50 Batch 40/97] Loss: 0.7695\n",
            "[Epoch 31/50 Batch 50/97] Loss: 0.6366\n",
            "[Epoch 31/50 Batch 60/97] Loss: 0.6440\n",
            "[Epoch 31/50 Batch 70/97] Loss: 0.7448\n",
            "[Epoch 31/50 Batch 80/97] Loss: 0.7003\n",
            "[Epoch 31/50 Batch 90/97] Loss: 0.6299\n",
            "[Epoch 32/50 Batch 10/97] Loss: 0.6503\n",
            "[Epoch 32/50 Batch 20/97] Loss: 0.7185\n",
            "[Epoch 32/50 Batch 30/97] Loss: 0.6548\n",
            "[Epoch 32/50 Batch 40/97] Loss: 0.6741\n",
            "[Epoch 32/50 Batch 50/97] Loss: 0.6784\n",
            "[Epoch 32/50 Batch 60/97] Loss: 0.6557\n",
            "[Epoch 32/50 Batch 70/97] Loss: 0.7130\n",
            "[Epoch 32/50 Batch 80/97] Loss: 0.7005\n",
            "[Epoch 32/50 Batch 90/97] Loss: 0.6200\n",
            "[Epoch 33/50 Batch 10/97] Loss: 0.6941\n",
            "[Epoch 33/50 Batch 20/97] Loss: 0.6927\n",
            "[Epoch 33/50 Batch 30/97] Loss: 0.7428\n",
            "[Epoch 33/50 Batch 40/97] Loss: 0.7432\n",
            "[Epoch 33/50 Batch 50/97] Loss: 0.8240\n",
            "[Epoch 33/50 Batch 60/97] Loss: 0.6814\n",
            "[Epoch 33/50 Batch 70/97] Loss: 0.6172\n",
            "[Epoch 33/50 Batch 80/97] Loss: 0.5976\n",
            "[Epoch 33/50 Batch 90/97] Loss: 0.7399\n",
            "[Epoch 34/50 Batch 10/97] Loss: 0.7457\n",
            "[Epoch 34/50 Batch 20/97] Loss: 0.6480\n",
            "[Epoch 34/50 Batch 30/97] Loss: 0.6734\n",
            "[Epoch 34/50 Batch 40/97] Loss: 0.7089\n",
            "[Epoch 34/50 Batch 50/97] Loss: 0.6448\n",
            "[Epoch 34/50 Batch 60/97] Loss: 0.6116\n",
            "[Epoch 34/50 Batch 70/97] Loss: 0.7955\n",
            "[Epoch 34/50 Batch 80/97] Loss: 0.7170\n",
            "[Epoch 34/50 Batch 90/97] Loss: 0.6856\n",
            "[Epoch 35/50 Batch 10/97] Loss: 0.6366\n",
            "[Epoch 35/50 Batch 20/97] Loss: 0.7154\n",
            "[Epoch 35/50 Batch 30/97] Loss: 0.7009\n",
            "[Epoch 35/50 Batch 40/97] Loss: 0.7344\n",
            "[Epoch 35/50 Batch 50/97] Loss: 0.7019\n",
            "[Epoch 35/50 Batch 60/97] Loss: 0.6279\n",
            "[Epoch 35/50 Batch 70/97] Loss: 0.6471\n",
            "[Epoch 35/50 Batch 80/97] Loss: 0.6252\n",
            "[Epoch 35/50 Batch 90/97] Loss: 0.6258\n",
            "[Epoch 36/50 Batch 10/97] Loss: 0.6265\n",
            "[Epoch 36/50 Batch 20/97] Loss: 0.6997\n",
            "[Epoch 36/50 Batch 30/97] Loss: 0.7304\n",
            "[Epoch 36/50 Batch 40/97] Loss: 0.6952\n",
            "[Epoch 36/50 Batch 50/97] Loss: 0.6865\n",
            "[Epoch 36/50 Batch 60/97] Loss: 0.6673\n",
            "[Epoch 36/50 Batch 70/97] Loss: 0.7430\n",
            "[Epoch 36/50 Batch 80/97] Loss: 0.7224\n",
            "[Epoch 36/50 Batch 90/97] Loss: 0.6396\n",
            "[Epoch 37/50 Batch 10/97] Loss: 0.6588\n",
            "[Epoch 37/50 Batch 20/97] Loss: 0.7836\n",
            "[Epoch 37/50 Batch 30/97] Loss: 0.6470\n",
            "[Epoch 37/50 Batch 40/97] Loss: 0.6637\n",
            "[Epoch 37/50 Batch 50/97] Loss: 0.6433\n",
            "[Epoch 37/50 Batch 60/97] Loss: 0.6746\n",
            "[Epoch 37/50 Batch 70/97] Loss: 0.6292\n",
            "[Epoch 37/50 Batch 80/97] Loss: 0.6833\n",
            "[Epoch 37/50 Batch 90/97] Loss: 0.6733\n",
            "[Epoch 38/50 Batch 10/97] Loss: 0.6937\n",
            "[Epoch 38/50 Batch 20/97] Loss: 0.6122\n",
            "[Epoch 38/50 Batch 30/97] Loss: 0.6770\n",
            "[Epoch 38/50 Batch 40/97] Loss: 0.6620\n",
            "[Epoch 38/50 Batch 50/97] Loss: 0.7006\n",
            "[Epoch 38/50 Batch 60/97] Loss: 0.7074\n",
            "[Epoch 38/50 Batch 70/97] Loss: 0.7104\n",
            "[Epoch 38/50 Batch 80/97] Loss: 0.6689\n",
            "[Epoch 38/50 Batch 90/97] Loss: 0.6608\n",
            "[Epoch 39/50 Batch 10/97] Loss: 0.6809\n",
            "[Epoch 39/50 Batch 20/97] Loss: 0.6948\n",
            "[Epoch 39/50 Batch 30/97] Loss: 0.7639\n",
            "[Epoch 39/50 Batch 40/97] Loss: 0.7142\n",
            "[Epoch 39/50 Batch 50/97] Loss: 0.6367\n",
            "[Epoch 39/50 Batch 60/97] Loss: 0.6522\n",
            "[Epoch 39/50 Batch 70/97] Loss: 0.6847\n",
            "[Epoch 39/50 Batch 80/97] Loss: 0.6875\n",
            "[Epoch 39/50 Batch 90/97] Loss: 0.6046\n",
            "[Epoch 40/50 Batch 10/97] Loss: 0.6602\n",
            "[Epoch 40/50 Batch 20/97] Loss: 0.7324\n",
            "[Epoch 40/50 Batch 30/97] Loss: 0.6255\n",
            "[Epoch 40/50 Batch 40/97] Loss: 0.6791\n",
            "[Epoch 40/50 Batch 50/97] Loss: 0.5885\n",
            "[Epoch 40/50 Batch 60/97] Loss: 0.7088\n",
            "[Epoch 40/50 Batch 70/97] Loss: 0.6681\n",
            "[Epoch 40/50 Batch 80/97] Loss: 0.6796\n",
            "[Epoch 40/50 Batch 90/97] Loss: 0.6853\n",
            "[Epoch 41/50 Batch 10/97] Loss: 0.6117\n",
            "[Epoch 41/50 Batch 20/97] Loss: 0.6762\n",
            "[Epoch 41/50 Batch 30/97] Loss: 0.6738\n",
            "[Epoch 41/50 Batch 40/97] Loss: 0.7430\n",
            "[Epoch 41/50 Batch 50/97] Loss: 0.6674\n",
            "[Epoch 41/50 Batch 60/97] Loss: 0.6355\n",
            "[Epoch 41/50 Batch 70/97] Loss: 0.6310\n",
            "[Epoch 41/50 Batch 80/97] Loss: 0.7018\n",
            "[Epoch 41/50 Batch 90/97] Loss: 0.6653\n",
            "[Epoch 42/50 Batch 10/97] Loss: 0.6122\n",
            "[Epoch 42/50 Batch 20/97] Loss: 0.6667\n",
            "[Epoch 42/50 Batch 30/97] Loss: 0.6203\n",
            "[Epoch 42/50 Batch 40/97] Loss: 0.6707\n",
            "[Epoch 42/50 Batch 50/97] Loss: 0.6783\n",
            "[Epoch 42/50 Batch 60/97] Loss: 0.7072\n",
            "[Epoch 42/50 Batch 70/97] Loss: 0.6805\n",
            "[Epoch 42/50 Batch 80/97] Loss: 0.6010\n",
            "[Epoch 42/50 Batch 90/97] Loss: 0.6545\n",
            "[Epoch 43/50 Batch 10/97] Loss: 0.6827\n",
            "[Epoch 43/50 Batch 20/97] Loss: 0.6770\n",
            "[Epoch 43/50 Batch 30/97] Loss: 0.6716\n",
            "[Epoch 43/50 Batch 40/97] Loss: 0.6324\n",
            "[Epoch 43/50 Batch 50/97] Loss: 0.7017\n",
            "[Epoch 43/50 Batch 60/97] Loss: 0.6675\n",
            "[Epoch 43/50 Batch 70/97] Loss: 0.6793\n",
            "[Epoch 43/50 Batch 80/97] Loss: 0.6371\n",
            "[Epoch 43/50 Batch 90/97] Loss: 0.7345\n",
            "[Epoch 44/50 Batch 10/97] Loss: 0.6174\n",
            "[Epoch 44/50 Batch 20/97] Loss: 0.6963\n",
            "[Epoch 44/50 Batch 30/97] Loss: 0.6044\n",
            "[Epoch 44/50 Batch 40/97] Loss: 0.6723\n",
            "[Epoch 44/50 Batch 50/97] Loss: 0.5985\n",
            "[Epoch 44/50 Batch 60/97] Loss: 0.6506\n",
            "[Epoch 44/50 Batch 70/97] Loss: 0.6788\n",
            "[Epoch 44/50 Batch 80/97] Loss: 0.6716\n",
            "[Epoch 44/50 Batch 90/97] Loss: 0.7356\n",
            "[Epoch 45/50 Batch 10/97] Loss: 0.6321\n",
            "[Epoch 45/50 Batch 20/97] Loss: 0.7132\n",
            "[Epoch 45/50 Batch 30/97] Loss: 0.6368\n",
            "[Epoch 45/50 Batch 40/97] Loss: 0.6420\n",
            "[Epoch 45/50 Batch 50/97] Loss: 0.6849\n",
            "[Epoch 45/50 Batch 60/97] Loss: 0.7829\n",
            "[Epoch 45/50 Batch 70/97] Loss: 0.7493\n",
            "[Epoch 45/50 Batch 80/97] Loss: 0.6052\n",
            "[Epoch 45/50 Batch 90/97] Loss: 0.6912\n",
            "[Epoch 46/50 Batch 10/97] Loss: 0.6240\n",
            "[Epoch 46/50 Batch 20/97] Loss: 0.6739\n",
            "[Epoch 46/50 Batch 30/97] Loss: 0.7247\n",
            "[Epoch 46/50 Batch 40/97] Loss: 0.6619\n",
            "[Epoch 46/50 Batch 50/97] Loss: 0.6605\n",
            "[Epoch 46/50 Batch 60/97] Loss: 0.6818\n",
            "[Epoch 46/50 Batch 70/97] Loss: 0.6083\n",
            "[Epoch 46/50 Batch 80/97] Loss: 0.7135\n",
            "[Epoch 46/50 Batch 90/97] Loss: 0.7114\n",
            "[Epoch 47/50 Batch 10/97] Loss: 0.6842\n",
            "[Epoch 47/50 Batch 20/97] Loss: 0.6994\n",
            "[Epoch 47/50 Batch 30/97] Loss: 0.6297\n",
            "[Epoch 47/50 Batch 40/97] Loss: 0.6190\n",
            "[Epoch 47/50 Batch 50/97] Loss: 0.6461\n",
            "[Epoch 47/50 Batch 60/97] Loss: 0.6290\n",
            "[Epoch 47/50 Batch 70/97] Loss: 0.6129\n",
            "[Epoch 47/50 Batch 80/97] Loss: 0.6190\n",
            "[Epoch 47/50 Batch 90/97] Loss: 0.6971\n",
            "[Epoch 48/50 Batch 10/97] Loss: 0.6881\n",
            "[Epoch 48/50 Batch 20/97] Loss: 0.6457\n",
            "[Epoch 48/50 Batch 30/97] Loss: 0.6511\n",
            "[Epoch 48/50 Batch 40/97] Loss: 0.6850\n",
            "[Epoch 48/50 Batch 50/97] Loss: 0.6754\n",
            "[Epoch 48/50 Batch 60/97] Loss: 0.7221\n",
            "[Epoch 48/50 Batch 70/97] Loss: 0.6719\n",
            "[Epoch 48/50 Batch 80/97] Loss: 0.6403\n",
            "[Epoch 48/50 Batch 90/97] Loss: 0.6875\n",
            "[Epoch 49/50 Batch 10/97] Loss: 0.6683\n",
            "[Epoch 49/50 Batch 20/97] Loss: 0.7617\n",
            "[Epoch 49/50 Batch 30/97] Loss: 0.6772\n",
            "[Epoch 49/50 Batch 40/97] Loss: 0.6817\n",
            "[Epoch 49/50 Batch 50/97] Loss: 0.6329\n",
            "[Epoch 49/50 Batch 60/97] Loss: 0.6507\n",
            "[Epoch 49/50 Batch 70/97] Loss: 0.5756\n",
            "[Epoch 49/50 Batch 80/97] Loss: 0.5526\n",
            "[Epoch 49/50 Batch 90/97] Loss: 0.6189\n",
            "[Epoch 50/50 Batch 10/97] Loss: 0.6597\n",
            "[Epoch 50/50 Batch 20/97] Loss: 0.7003\n",
            "[Epoch 50/50 Batch 30/97] Loss: 0.5869\n",
            "[Epoch 50/50 Batch 40/97] Loss: 0.6763\n",
            "[Epoch 50/50 Batch 50/97] Loss: 0.6641\n",
            "[Epoch 50/50 Batch 60/97] Loss: 0.6834\n",
            "[Epoch 50/50 Batch 70/97] Loss: 0.7099\n",
            "[Epoch 50/50 Batch 80/97] Loss: 0.6987\n",
            "[Epoch 50/50 Batch 90/97] Loss: 0.6478\n",
            "Test Accuracy: 0.8109\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}